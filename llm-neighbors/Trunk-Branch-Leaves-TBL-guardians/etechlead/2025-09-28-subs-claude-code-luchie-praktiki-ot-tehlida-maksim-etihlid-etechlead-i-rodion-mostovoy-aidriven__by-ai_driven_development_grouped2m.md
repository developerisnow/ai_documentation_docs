[00:00:00] Ещё раз, ещё раз привет всем. Вот мы собрались на наш замечательный татап поткоду, по кодексу. Мы сейчас так немножко там вглубь полезем смотреть, как они с контекстом работают, э, как нам самим эффективнее и лучше с этим работать. А по формату, по формату, пожалуйста, вопросы можете в чатик прямо закидывать, прям��, в принципе, по ходу движения. А сразу не обещаем, что ответим. а-а в, скорее всего, будут какие-то чекпоинты, да, на которых будем отвечать на вопросы. Если прямо очень будет большое желание у кого-то голосом задать вопрос, то, в принципе, тоже такое возможно. Мы в зуме всё-таки, да? Поднимайте руку. Поднимайте руку и тогда там пообщаемся. Всё, спасибо, Максим, тебе слово. Максим у нас, да, у него там свой свой тоже канал, я думаю, многие знают, да, по про разработку, как раз про яве разработки. И у Максима очень классная, великолепная презентация он нам подготовил. Вот Максим, жги. Угу. Так, значит, смотрите, а наша сегодняшняя, а, ну, презентации воркшоп посвящены лод-коду и агентам в целом. Конкретно мы будем рассматривать именно агентов, которые консольные агенты, как ни странно. Вот. И начнём с общего определения агента для того, чтобы было понятно, да, е, чтобы для начала это агенты, которые используются для написания кода. То есть это кодовые агенты. Я буду исходить из того, что у нас агентом является некая система, которая состоит из нейронки, а, некого программного продукта, привязанного к этой нейронке, и некоторого окружения, которое с которым нейронка взаимодействует через вот этот вот набор инструментов, который предоставляется непосредственно самим

[00:02:00] программным продуктом. Ну, есть ещё человек. Это классическое определение агента. то, как его трактует антропик. То есть, фактически на каждом, а, этапе своего своей работы вся эта система, а, при помощи LLM предпринимает некое действие, которое воздействует на окружающую среду, а от окружающей среды поступает какой-то какая-то обратная связь. Она может быть в любой форме, ну, это не принципиально. И в зависимости от того, какова эта обратная связь, LLM принимает решение о том, какой следующий экшн совершить при помощи тулинга или принимает решение о том, что можно закончить работу. Вот где-то вот тут вот посередине должен быть как раз тот самый программный продукт, про который я говорю. То есть это должен быть непосредственно сам акo-код там или кодекс или ещё какая-нибудь э то есть тот тот же самый курсор, там есть у него внутренний агент. И, соответственно, вот здесь вот они а и работают. То есть они связывают фактическим, а, с окружением, позволяя ей выполнять определённые действия и давая ей фидбэк. Вот, соответственно, агент - это такая бол��е-менее интеллектуальная система, которой на каждом своём этапе даёт свободу выбора, какой инструмент использовать и что конкретно делать дальше. Вот. И это отличается от классического определения, а от, точнее, от классических систем, с которыми, возможно, вы сами работали. Они основаны на Workflows. Вот Workflows, они действуют по заранее заданной схеме. И даже если ваш workflow включает в себя LM, это не делает его агентом. А, и, например, бывает workflow, в которых LM присутствует на многих этапах. А, к примеру, там написала код, дальше передали какому-нибудь ревюевееру. Дальше, может быть, ещё какой-нибудь secкюрити ревюер

[00:04:00] подключился, то есть или, может быть, там отдельный процесс, который собрал информацию о системе и так далее. Но всё это выстраивается в виде чётких последовательностей, которые так или иначе жёстко заданы вами самими. Это workflow, а, а вот здесь вот сверху это агент. Опять-таки, а я ориентируюсь на определение тропик и терминологических споров в коммьюнити море. Если хотите, можно на это потратить несколько часов и всё равно не прийти к единому выводу, что же, собственно, такой агент, но будем исходить из этого определения и использовать его в качестве основного. Вот как вообще устроен сам по себе агент? А агент, вот если рассматривать именно ту самую программную вот систему, которая включает в себя LLM, там включает в себя вот этот вот некий посредник, который стоит между м и окружением, то это вот как раз наши любимый код-код и кодекс, про который, на основе которых я буду рассказывать вообще в целом про агенты. Давайте, может быть, пару слов скажу, почему вообще все агенты стали так популярными. Ну, тут есть несколько вещей. То есть, во-первых, они не привязаны к UI, и это позволяет гораздо быстрее двигаться им в плане развития. А почему это быстрее? Потому что, а, та же самая Vжусди код, несмотря на то, что на ней построено очень много как раз кодовых агентов. А тот же самый курсор, а огмен, а VРF, они все так или иначе завязаны на фишке Visual Stud Code, а кастомизируемость её в плане добавления в неё заранее незаложенных фич за пределами точек кастомизации, которая сама Visual Studio Code предоставляет, довольно-таки ограничена. Это во-первых. А, во-вторых, есть ещё проблемы с тем, что, а, эта ограниченность выражается не только

[00:06:00] в том, что вы, то есть не только в том, что вам нужно каким-то обра��ом привязываться в QI, а там, в принципе, есть некоторые моменты, которые обладают некоторым вендерлоком со стороны Microsoft. То есть не всё получается использовать в этих форках. Вот. Ну и мы постепенно приходим к тому, что по идее непосредственно сама ID для генерации кода, она как бы и не особо-то нужна для просмотра кода, да, а вот непосредственно для того, чтобы код сам сгенерировать, ну как бы и не особо-то и надо получается. И соответственно люди, кто вот занимался кодовыми агентами, ну первый, кто, пожалуй, сделал такого вот консольного кодового агента - это как раз antropic clot код. И они просто сделали консольный вариант, потому что, ну, это было быстрее. Вот. И, соответственно, вот эта вот агентская составляющая, которая по сути является центровой в самом агенте, само ядро, оно просто быстрее развивается за счёт того, что они дропнули поддержку ГУИ и просто решили делать консольный клиент. Там интерфейсы гораздо более простые, и, соответственно, они могут уделить время больше непосредственно развитию самой агентской составляющей. Вот. Второе - это кастомизируемость. Как мы дальше увидим, а кастомизация вот именно таких консольных агентов заключается в том, что они позволяют встраиваемость их в разного рода пайплайна, который вы делаете сами. Вот. Потому что запустить его можно где угодно, а можно разные сценарии использования интегрировать. То есть его интегрируются, разные сценарии использования. И плюс ко всему самом непосредственно инструменте за счёт того, что как раз-таки он быстрее двигается, там появляются некоторые возможности, которых вы, скорее всего, в UI-редакторах-то и не увидите, просто потому что они вот именно в рамках консольного редактора быстрее реализуемые. Вот. Ну и ещё плюс ко всему есть вот в плане встраиваемости есть такая штука, что аа SDK - это как раз вы можете прямо в свои собственные программы встраивать вызов

[00:08:00] клода. А есть возможность делать фоны в агенты. Это как раз такие вещи, которые работают, например, в облаке, когда в случае с тем же самым кодексом вы можете зайти на сайт chatpt и запустить там вашего агента, который будет что-то делать аа независимо от вас, а где-то в контейнере, который исполняется в отдельном окружении, опять-таки в облаке. То же самое работает для GitHub агента. И то же самое можно настроить даже для код-кода, запуская его поверх инфраструктуры GitHub Actions. А если захочется, даже ей в GitLab можно запус��ить. Вообще где угодно можно запустить, если уж на того пошло. Вот, соответственно, у нас появляется возможность создавать некоторые такие программируемые workflows поверх агентов, которые встраиваются в общую конву вашего процесса разработки. Ну и это ещё породило целый ряд инструментов новых. Аэ, точнее сделало возможным гораздо более простое их внедрение. аа такие вещи, как Specdrive Development, и есть вот некоторые из его реализаций. Эта тема сама по себе отдельная, большая, про неё нужно прямо отдельные доклады делать. Вот. А есть вот реализации Spec Kit и BMAT. И, например, такой продукт, как VIP can, он тоже стал возможным, но вот только лишь из-за существования консольных агентов. Это такая штука, которая вам позволяет на своей машине, а, фактически сделать некоторую канбан борду, а, накидать туда задачи и запускать эти задачи на исполнение тем агентом, который у вас на машине развёрнут. То есть клодкодом или кодексом там или квенкодером, как как захоти��е. То есть вот а можно прямо у себя на машине сделать некий таск-трекер, который непосредственно привязан к программирующему агенту. Всё это у вас на машине будет работать. В принципе, можно и удалённо запустить, но суть в том, что главное, что здесь именно есть возможность вызова кодового агента как консольного инструмента, которому вот просто передаются задача, и он с ней работает. Вот это довольно прикольно,

[00:10:00] то, что вообще в принципе такое возможно, и вы не ограничены конкретно этим инструментом, вы можете делать свои. Благо, что порог вхождения тут довольно-таки низкий сейчас вот так. Может быть, к этому моменту есть какие-то вопросы насущные? >> Так, Максим, ну, тут вопросы появляются постепенно. Так, означает ли ограниченность точек расширения просто задержку э в интеграции новых фич VSCД? >> Да, определённо. И не, скажем, с одной стороны это задерживает, да, интеграцию, а с другой стороны я не ��верен теперь уже, что некоторые из этих интеграций вообще там появятся. То есть либо у нас появится какой-то инструмент, который будет, возможно, иметь в базе Visual Stud CД, но визуально он будет прямо очень сильно отличаться от того, к чему мы привыкли. Посмотрите ради интереса на KO от Амазона. Ну, довольно сильно отличается от классических ID. Угу. Так, то был вопрос от Ильи. Теперь вопрос от Артёма. Разве Spec Driven Development - это не калька с того, как ведётся разработка людьми в командах? Имею в виду Useries, пеки разработка, ведь процесс-то известен. >> Да, так и есть. Очень похоже на то, как э всё это работает с людьми, за исключением того, что тут очень многие процессы, точнее, очень многие этапы этого процесса становятся автоматизируемыми, вот, и верифицируемыми, в том числе автоматически. Вы можете заранее фактически прописать всю систему там и, да, просто двигаться при помощи агентов по этому всему. А тогда очень похоже, конечно, это кла��сической такой, а, околовотопатный процесс. Ну да, ну только в том случае спека писалась для людей, да, а теперь получается спека пишется для агента, >> да, с учётом всех их возможных ограничений и преимуществ. Да. >> Да. Хорошо, спасибо, Максим. Ну давай дальше.

[00:12:00] >> Так, поехали дальше. Ну и вот из чего состоит вообще агент. А, ну агент, как я и говорил уже, ему не обязательно иметь вообще какой-то ГУИ. А, и по сути агент, э, принимает задачи из внешнего мира. и вообще, в принципе, отображать что-то во внешний мир при помощи некого интерфейса. Интерфейс может быть, как мы уже выяснили, гуишный, курсор, и так далее. Вот он может быть консольным. Это clкод, кодекс, кнкоoder, а, gemна ci. И сейчас их вообще уже я потерял счёт, их штук 15, наверное, уже появилось разных. Вот. И есть возможность вызывать аа этого же агента при помощи какого-то SDK, то есть программно. прямо можете в свои программы встраивать агентов, которые будут писать программы. Вот. И, соответственно, через этот интерфейс он обща��тся с внешним миром, а получает от вас, например, какие-то задания. А это задание передаётся оркестратору, который, в принципе, как раз и является ядром системы в том плане, что он общается с моделью и в таком цикле просто фактически направляет запросы либо, точнее, получает запросы интерфейса, опрашивает модели, вызывает инструменты, общается с памятью и так далее. То есть это как бы такая, э, можно сказать, а-а, цикл а-а, в котором вот как раз работает ор��естратор. Он просто вот по по вот этой схеме кругам бегает. Потом дальше посмотрим. Но суть в том, что он организовывает общение с моделями где-то в облаке, как правило, а, с локальными моделями тут не имеет значения. Суть в том, что он каким-то образом общается с моделью, которая, а, непосредственно является мозгом всей этой системы. то есть с LLM. И у него есть некий набор инструментов, который реализован в самом вот этом программном инструменте, то есть в самом колод-коде, а, или в кодексе, а, который позволяет ему общаться с внешним миром непосредственно там, где этот агент запущен. А внешним

[00:14:00] миром, что для него может являться файловая система, а или, например, тот же самый SH, то есть именно программная оболочка, которая командная оболочка, которая в конкретной системе развёрнута. Или вот MCP тоже горячая тема. А спустя уже почти год после выхода протокола всё всё более и более горячей становится. Суть в том, что агент тоже может не только к локальной системе обращаться, он, в принципе, через MCP может обращаться куда угодно и давать какие угодно возможности непосредственно вот вашему агенту а через LM а она может практически что угодно делать, если у вас есть соответствующий MCP-сервер. Вот, соответственно, оркестратору даётся набор инструментов. Вот они, кстати, в случае с код-кодом. Здесь вот некоторые из них перечислены. И, как вы видите, они довольно-таки простые в базе своей, то есть там, например, прочитать файл или погрепать, а, то есть найти какой-то определённый файл среди других файлов или найти определённую строчку в конкретном файле. Вот вызов баш, а, непосредственно какого-то командно-срочного интерфейса. И самый интересный из них, который нам понадобится дальше - это Task. Task - это запуск внутреннего, ну, можем, можно сказать, потока подпроцесса, но скорее потока, да, это именно поток внутри основного исполнения, который позволяет, а, подзапустить некий, а, внутренний аа внутреннюю копию самого агента. Она такая минималистичная, которую мы называем собагентами. То есть вот инструмент task, он как раз об этом. Он позволяет вам выделить часть функционала, часть задачи, которая вот большой задачи, которая пришла, аа её часть после декомпозиции выделить в отдельную маленькую подзадачу и передать её собагенту. Собагент дальше ей будет заниматься. Мы об этом подробнее

[00:16:00] поговорим. Вот, соответственно, это вот набор инструментов и память. А память нужна оркестратору и, в принципе, всему агенту для того, чтобы понимать, а что вообще вокруг происходит и что происходит в процессе его работы. Он туда какие-то временные данные, в том числе складывает. Но начнём с того, что у у агента есть такие вещи, как MD и Agents MD. Это такие штуки, ну, это файлы, просто обычные Markдау файлы, которые вы кладёте себе в проект, и они на верхнем уровне описывают то, как проект устроен, какие основные, аэ, вещи нужны для его запуска, на каких технологиях он построен, зачем вообще проект нужен и так далее. И это довольно-таки базовый документ, который описывает чисто общую канву вашего проекта. Он не предназначен для того, чтобы вы туда всю информацию о проекте клали. А это даже вредно. То есть он не должен быть у вас слишком большим, он именно должен быть, э, довольно-таки минималистичным, чтобы агенту было понятно, с чего начать. А дальше агент идёт, э, например, в файлы проекта, и уже по файлам проекта проходится для того, чтобы собрать непосредственно информацию для реализации какого-то функционала. а-а он прямо конкретно в каждый файл проекта посмотрит, который относится так или иначе аа к тому, что ему нужно сделать, и уже в него будет вносить изменения. Соответственно, память в том числе и в проектных файлах у вас содержится. Причём эти проектные файлы могут быть не обязательно файлами кода. Это могут быть файлы документации, которые вы точно также положили в проекте. А, и в процессе своей работы вот оркестратор, пока он ходит по вот этим разным файлам, связанным с памятью, файлом, связанным с проектами, он фактически собирает контекст. Ему этот контекст нужен для исполнения задачи, как перед началом работы, так и в процессе работы он может этот контекст начать собирать.

[00:18:00] Соответственно, а для вас очень важно, а, для конкретной задачи дать ему ровно столько контекста, сколько ему нужно для исполнения этой задачи, и в том числе не дать больше, чем ему нужно, потому что иначе возникают проблемы, про которые мы тоже сейчас чуть побольше поговорим. А если вы дадите агенту больше контекста, чем нужно, он у вас, скорее всего, будет очень плохо работать. И если дадите меньше, он сделает не то, что вам надо. Соответственно, вот это вот очень такой тонкий баланс между тем, чтобы дать только столько, сколько нужно, и не дать сверх этого для того, чтобы агент точно выполнил то, что вы от него хотите. И в этом случае довольно-таки интересными являются системы, которые позволяют, а, контекст для агента собирать ему не самому, при помощи вот этих вот инструментов, когда он просто ходит по проекту, например, при помощи ГЭПа ищет необходимый файл. А есть инструменты гораздо более высокого уровня. Вот возьмём тот же самый курсор. Там есть индексация кодовой базы, например, но она довольно примитивно устроена, и курсор иногда промахиваются аэ с тем, где же, собственно, найти конкретный файл, который нужно изменить для того, чтобы добавить нужный функционал. Или, предположим, мы возьмём тот же самый клод-код, у него вообще ничего нету в плане индексации вашего проекта. Он просто каждый раз для него проект как новый. А-а, и он каждый раз заново будет пробегать по все по файлам проекта, вычитывая их для того, чтобы сформировать необходимый контекст. Это проблема, потому что на это тратится время, и этот процесс гораздо менее точный, чем если бы мы использовали какие-то серьёзные средства для как раз непосредственно работы с контекстом. такие средства появляются. И одно из них, ну, точнее, как они появляются встроенными в конкретные агенты. То есть в том же самом курсоре, вот мы знаем, а мы знаем, что есть такойment code. А

[00:20:00] есть такая штука в кодер, это Q кодер, такая вот недавно появившаяся китайская ID. В ней тоже есть некий контекст engine, который позволяет вам заранее собрать информацию о проекте. И фактически в ответ на запрос на естественном языке под конкретную задач�� он агенту возвращает необходимый набор файлов, с которым нужно работать. Вот. А агент уже в них вносит изменения. И есть вот такие вот штуки, как коды life, которые по сути представляет собой внешний по отношению к агенту а процесс аа облачный сервис, который индексирует контекст, а индексирует ваш проект и предоставляет контекст вашему агенту для того, чтобы он смог эффективно с ним работать. Тут я передам слово Радион, он про него более подробно расскажет. Ну да, и скрыва��ь не бу. Спасибо, Максим. Скрывать не будем. Вот то, что Максим упомянул, кода - это вот как раз проект моей команды, поэтому я так высокоуровнево немножко про него расскажу. Потом, если будет интересно, могу подробнее рассказать, э как вообще это всё появилось, да. А, ну вот начнём как бы от проблемы, да, как полагается. А вот сейчас у этих кодовых агентов наверное, существуют такие две э пожалуй такие фундаментальные проблемы. А первая проблема возникает при работе с большими кодовыми базами, да, проблема с контекстом, когда кодагент, ну, просто банально не подтягивает весь необходимый контекст, да, он что-то там погрепал, что-то собрал, плот-код довольно неплохо работает в целом, но тем не менее, как там, чем больше кодовая база, тем выше вероятность того, что он упустит нужные, а, нужные места, начнёт где-то промахиваться, что-то выдумывать, какие-то фазы создавать дублирующие и так далее, да. Ну, это вот такая первая проблема. Ну и вторая проблема здесь достаточно важная, это возможность работы с, ну вот, скажем так, с

[00:22:00] мультирепозиториями, да? То есть когда в, опять же, это больше актуально для там больших команд, э когда условно там идёт разделение на там фронтend, backend, мобильное приложение, может быть ещё там на множество микросервисов подразделяется, и всё это в разных репозиториях. И в принципе это всё взаимосвязано. Вот. И вот это вот такая начинается интересная такая здесь ээ э интересная история. То есть мы как только берём какую-то фичу или может быть даже какой-то баг простой, а зачастую это всё размазано по, ну, как минимум, там по нескольким, да, вот этим модулям, по нескольким репозиториям. И, конечно, полноценно вот здесь вот, скажем так, заюзать вот эти агенты код, кодекс, ну становится, ну, существенно сложнее, да, эта задача, она сложность очень сильно вырастает. А, и вот у нас такая гипотеза возникла, что, а, что, если, собственно говоря, сделать эту систему индексации кода и не просто, скажем так, вот векторизовать, да, вот кто знает, да, не просто вот вектора там посчитать для cд ss, для функций, для классов, а ещё и разобрать это всё на связи, да? А что тут за граф вызовов такой? А какая как иера археотипов, да? А, и в общем, вот это всё собрать, а, и объединить это в такой вот, э- в такую единую базу знаний, скажем так, да, из этого всего. Это вот кто тоже кто в теме, это что-то похоже на граф граф рак, да, вот такое вот, а, в единую базу знаний. При этом ещё и добавить возможность совмещать это там множество репозиториев в эту базу знаний. А вот таким вот образом, да, родилась идея создать. И в принципе довольно успе не быстро, довольно долго мы это сделали, но тем не менее вот уже там к апрелю, собственно, этого года продукт был опубликован, да, вот он сейчас доступен, есть есть облачная версия, есть, кстати, он премрсия, что тоже большая редкость, да, для таких продуктов, как раз вот для больших компаний. >> А, и вот тут как бы, да, ключевая идея в том, чтобы обогатить контекст кодовых

[00:24:00] агентов, ээ, вот заранее проиндексированной вот этой кодовой базой. То есть кодовый агент, прежде чем начать какую-то работу, он, скажем так, консультируется, да, с контекстным движком. Он говорит: "Я такую-то хочу сделать работу, мне тут надо аудентификацию поменять". Он отправляет запрос контекстному движку. Ээ контекстный движок собирает быстро информацию. Потому почему быстро? Потому что у него предондексация уже, да, совершилась. То есть, в принципе, поскольку мы можем оперировать целыми графами, да, вот прямо вот можем прямо вытаскивать кусок, вот он тификация шок не относится. вытаскиваем кусок относящийся нотификации, вот там граф вызовов, а вот эти связи и довольно быстро возвращаем, да, соответственно, какой из этого профит? Ну, как, во-первых, как я уже сказал, это повышение осведономённости кодового агента. А, и второе - это на самом деле повышение скорости работы тоже, да, потому что мы сокращаем и помимо того, что больше контекста, мы ещё множество шагов сокраща сокращаем. А, ну это вот вот это как раз вот пример контекста за сервис. Сейчас вот потихонечку такие сервисы тоже там начинают появляться. Э, но вот мы как бы предвосхитили такой вот эту моду. Поэтому, соответственно, к к любому продукту там, да, к любому кодагенту в любые пайплайны, сейчас у нас компания в свои пайплайны, кто-то в кодрев встраивает это, да, чтобы контекст обогатить. Аэ, вот есть есть такое решение. Посмотрите, их сейчас действительно не так много. Ну вот можете прямо даже, если будет интересно с нашего начать. такая. Мы с вами внутри используем, в общем, прям очень довольны. >> Ну да, >> спаси��о. >> Да, Максим, спасибо. Спасибо тебе, Максим, да, за этот момент пича. Значит, смотрите, да, я я скину ссылочку. Максим, можешь, пожалуйста, пока что ссылочку поширить? А я просто пока тебя немножко дополню про этот ClD. Мне кажется, важно здесь сказать. что ты показывал, что, ну, смотрите, друзья, в принципе, вот MD - это такая своего рода некая попытка, а-э, сделать вот эту самую индексацию, ну,

[00:26:00] такую простенькую какую-то, да, но вот такую преиндексацию, вот этот файлик clдомd. И вот Максим говорил о том, что желательно их не чтобы он не распухал этот clдм MD. Почему? Ну, потому что мы понимаем, что контекст очень важен, размер контекста очень важен, его желательно экономить. А, и вот здесь вот Максим тоже сказал о том, что внутри проектов, в этом плане, да, в этом плане, внутри проектов нужно создавать клодом D, да. Вот это вот важный такой момент, что там у нас есть где-то там наверху, на верхнем уровне в в корне, да, а там ClДМ MD, Agents MD и по-хорошему вот дальше там в уровнях пониже, если у вас там множество проектов, если у вас подпроекты, по-хорошему, Максим, если я правильно понимаю, по прав, если не прав, то в желательно вот в каждые вот эти подпроекты тоже свои вот эти ёмкие такие clды MD размещать. >> Да, всё верно, да. Да, Максим, да, спасибо. Тут вот ещё был вопрос хороший. Сейчас я пока пока пичил немножко. Сейчас я упустил чат. Сейчас одну секунду. Хороший был вопрос тоже. А вот просто, да, классный вопрос. Clмт подаётся на вход с каждым запросом. Верно. >> А и вот тут хороший комментарий, что Андрей комментирует: "Да, только зачастую Клод всё равно игнорит его". Да. И вот и вот здесь Максим сейчас тоже поправь, если если я ошибусь. Насколько я понимаю, даже Clod MD может быть даже не в систем пром-то подаётся. Может быть, он даже и в USер подаётся. Тут вот, >> может, да, он не является частью системного промта, да? То есть системный промт передаётся с каждым запросом, но км MD не входит в системный промт. Это да. >> Да. Вот это вот важный момент. И, друзья, если вдруг у кого-то возникнет большое желание, ну, прямо вот лайфхаком делюсь, а действительно пропачить клод вот так, чтобы он всегда учитывал ваше пожелание, скажем так, да, это тоже возможно сделать. Пожалуйста, такой реверсжениринг такой небольшой, да, здесь помощь клод распространяется вот

[00:28:00] таким, э, минифицированным фуцированным JS-файлом, в котором промт в чистом виде лежит. Вот. И его можно пропачить. Вот я пробовал, это действительно работает. И вот действительно, когда вы там systemт пачите, например, вы можете попросить его, чтобы он всегда какой-то MCP вызывал, да? Вот мы так делали, чтобы он всегда какола Life вызывал, например, и вот это уже будет работать, да? Вот поэтому и вот пользуйтесь, если кому-то будет нужно, такая есть возможность. >> Угу. Ну да. Так что дальше идём? >> Ну да, да, давай потихонечку дальше. Если вопросы интересные ещё там будут, давай я их зафиксирую. Давай дальше. >> Угу. Ну и вот, да, как Радион правильно сказал, вот я забыл упомянуть, действительно, вот ClМ MZ, они там поддерживают некую иерархию. Вот, соответственно, если у вас проект модуляризирован, а, или там просто хотя бы есть несколько подпапок с разным функционалом, имеет смысл код MD файлы создать в каждой из них, который бы описывал конкретную подведку проекта и или конкретный модуль. То есть это тоже можно делать, но это всё равно как бы такое в том плане, что а это накладывает на вас в том числе ещё ограничение по обновлению всего этого дела. То есть вы, а проект развивается, естественно, проекты не бывают статичными. Вот и, соответственно, вам нужно постоянно следить за тем, чтобы эти файлы были актуальными. Это отдельная задача сама по себе. Вам нужно предпринимать усилия потому, чтобы эту актуальность поддерживать. И в этом отношении, а, такие вещи, как контекст engine, они, конечно, вам а-а часть работает и с вас снимают за счёт того, что вот этот необходимый контекст, он где-то независимый существует. Вот. А всё-таки для агента каждый раз собирать вот этот вот контекст по проекту, ну, это правда, это и долго, и, в общем, напряжно, то есть такое себе. Это ещё и дорого в том плане, что если вы не пользуетесь, например, подпиской, а, не дай бог пользуетесь непосредственно какими-то

[00:30:00] вот ам по их апе, но это ещё и деньги есть будет. Соответственно, каждый раз сбор такого контекста, и это просто деньги. Агент поглазит по проекту, пока вычитывает файлы, а, возможно, даже не те, которые ему нужны для конкретной задачи, он просто тратит ваши деньги. Вот об этом тоже надо всегда помнить. И следующий момент, который относится к памяти - это вот Memory Bank. Это такая концепция. Это именно методология, сразу скажу, как бы есть реализация этой методологии, но в целом это именно методология организации знаний о проекте непосредственно в вашем проекте, то есть прямо в самой кодовой базе. То есть Memory BН а в там в самом простом случае представляет из себя набор документов, которые описывают вашу систему с разных сторон. А он может описывать архитектуру, там подходы к UI дизайну, может быть какие-то вещи, относящиеся к тому, как код правильно выстраивать и может содержать в себе информацию по конкретным модулям и так далее. То есть, по сути, это методология, которая так или иначе описывает аа то, как проект устроен, как с ним работать и как самомуберибанку себя поддерживать. То есть в нём, в том числе, в этой методологии содержатся инструкции по тому, как делатьбан и как его поддерживать. При при том, когда у вас изменяется код самого проекта. Это всё не жёсткие вещи, это именно просто через промтинг указание агенту делать то или иное, в зависимости от того, что вам нужно для поддержания там или для вычитывания этого меory банка. Как правило, тот же самый Clod MD или Ag MD являются хорошей точкой входа в Memory Bank. Это в том числе и и ещё и поэтому стоит их держать минималистичными, потому что по-хорошему а расширенная информация о вашем проекте не должна лежать в этих файлах. Она должна лежать в файлах банка. Акod MD или JSM MD - это просто точка входа конкретно в Memory Bankн. То есть вам

[00:32:00] нужно фактически обеспечить некоторую некоторую навигацию из этого файла по файлам банка. То есть делаем ссылку, там агент дальше идёт в Memory Bank. Ещё раз. Memory Bank - это просто набор маркну файлов, вот, которые в самом самом проекте лежат. Ну и, соответственно, он просто эти файлы вычитывает в зависимости от того, а какой файл за что отвечает. Вы должны это просто пишать прописать в codd. А почему это, как я говорю, то есть я буду постоянно говорить, жёсткие, не жёсткие механизмы. Это не жёсткий механизм в том плане, что агент не обязан этого делать. Вы ему промтингом это д��ёте задание. Но так как у агента есть некоторая степень самостоятельности и забывчивости, он может и не прочитать то, что вы ему даёте. То есть тут ещё от конкретного агента, от конкретной ЛМ это всё очень сильно зависит. Это всё довольно-таки недодетерминировано. Вот. Ну, просто имейте в виду, что такая штука есть. И для небольших проектов её обязательно нужно использовать. То есть, если у вас есть вот такая задача обеспечить э информацию для агента о том, как проект устроен, Myриy BН - это отличная методология, её прямо стоит себе взять. Вот для тех вещей, которые побольше или там, где вам хочется сэкономить время, Contex Engine, да, это прямо вот тот класс инструментов, которые сейчас начинают появляться, они прямо очень нужны, особенно при работе с большими проектами. Вот. Потому что даже пока агент у вас лазит по проекту, он может, у него просто контекст может кончиться, на самом деле, по некоторым проектам там, ну, они просто слишком большие для того, чтобы агент сам мог эффективно собрать информацию по по этому проекту. Вот это, что касается анатомии агента. Так, ну, здесь-то, в принципе, нам всё понятно. Максим, давай, давай прерву тебя по по вопросам. Вот спрашивают примеры реализации банка, что самому поглядеть. Вот спрашивают. >> Угу. Как я говорю, это методология.

[00:34:00] >> Да, это методология. И если вам хочется классического её воплощения, посмотрите в CINE. У них там прямо в самом продукте реализован э Memory BН. Но это, опять-таки, я ещё раз повторюсь, это промтинг, и это не какой-то типа там готовая библиотека или что-то такое. То есть вам, по сути, нужно найти набор промтов для построения банка и для его поддержки. Вот. А дальше вы просто агенту в вашем конкретном проекте говорите: "Вот, вот тебе инструкция для построения банка, он вам его строит". И отдельно потом даёте ему инструкции: "А вот как работать с этим мемоory банком? Ну, откуда и как вычитываете файлы, как его обновлять?" И агент, соответственно, будет ещё и этим заниматься в процессе. То есть, ещё раз повторюсь, это не какой-то программный продукт, это просто набор инструкций для а вашего агента. Посмотрите вот на статью на сайте Клайна. И есть готовые оптимизированные под конкретные агенты наборы вот этих вот инструкций. Если вы просто погуглите Memory Bank, вы, я думаю, легко это найдёте. Вот. Так, идём дальше. Так, идём дальше, >> да? Давай ��альше. >> Ага. Так, а тогда давайте посмотрим вот просто на тактактактактак. Да, давайте посмотрим вот на то, как устроен Loop. А это как раз вот просто другой вариант э той диаграммы, которую мы видели раньше. Фактически у нас агент, а, получая на вход userпроompт, аа, проходит дальше некоторое количество этапов высокоуровневых, которые так или

[00:36:00] иначе приводят к финальному результату. Вот. Первым из таких этапов является планирование. Агенту действительно нужно собрать контекст по проекту, нужно посмотреть, как ту задачу, которую вы ему дали, декомпозировать на отдельные подзадачи и вообще, в принципе, начать её решать. Вот, соответственно, у него есть некоторый этап планирования, который в некоторых кодовых агентах реализован прямо отдельным инструментом. В том же самомкоде прямо есть возможность, и я настаиваю на том, что эту возможность нужно использовать при а при постановке задачи агенту именно заставить его планировать задачу задачу перед тем, как её начинать делать. В рамках планирования он соберёт необходимый контекст, куда дотянется, а запланирует, скажем, последовательность действий, которые нужно совершить, где что нужно поменять. И, что важно, даст вам возможность этот план поправить, а и просто сказав, что вас какой-то из моментов не устраивает, и потом двигаться дальше. А двигаться дальше у него фактически вот это вот просто цикл. Что-то делаем, проверяем, что сделали, что-то делаем. Проверяем, что сделали. Вот. И это как раз вот как как раз классический агентский луп, где он а даёт возможность lm принимать решение о том, какой инструмент вызывать следующим, и организовывает взаимодействие с внешним миром. То есть получает фидбэк. Вот. И, соответственно, просто вот в этом цикле всё движется до тех пор, пока не принимается решения о том, что результат достигнут и он выдаётся пользователю. То есть это вот как раз такой классический агентский лук цикл работы. И здесь вот как раз можем видеть, что промт инженеринг он вот на этом этапе находится, когда вы составляете пользовательский промт конкретно на постановку задачи. А контекст инженеринг он вот где-то вот тут появляется, то есть как раз он нужен для планирования. И агент, в том числе в процессе вот этого вот исполнения, он

[00:38:00] тоже периодически добирает контекст. То есть у него можно даже вот ещё вторую такую стрелочку сделать, на самом деле. То есть он, во-первых, в планировании собирает контекст, а во-вторых, он вот здесь вот ещё может этот контекст добирать для конкретного для конкретной подзадачи. Вот. Аа, соответственно, контекст инжениринг, именно процесс сбора контекста под конкретную подзадачу или под конкретный этап исполнения, он постоянно присутствует в цикле работы агента. Вот поэтому контекст инженеринг - это довольно-таки это важная штука, которая пронизывает весь всю цепочку работы агента. Вот поэтому об этом нужно всегда помнить. Так, тактактак. А, и давайте перейдём к таким более прикольным штукам в том плане, что мы погружаемся глубже. А то, как вообще устроен контекст, давайте об этом поговорим. И почему, почему вообще это важно? Почему мы столько вокруг контекста всего сейчас происходит? И почему вообще дисциплина контекст инжениринг появилась? Потому что контекст умок для начала ограничен. То есть у среднего размера лмок у нас сейчас контекст где-то в районе 200к токенов болтается. И это накладывает довольно-таки сильное ограничение на тот объём задач, который мы можем им давать. И это в том числе заключается как в количестве кода, которое они могут прочитать, в количестве документации, которые они могут прочитать и положить в контекст. Это, в том числе, влияет на то, сколько шагов может совершить при исполнении задачи, а сколько она всего каких-то инструментов может вызвать. Всё это так или иначе всё это влияет на контекст. То есть, пока вы вызываете инструменты, пока она собирает необходимые файлы по проекту, пока она читает вывод запущенных программ, пока вы ей дополнительные сообщения пересылаете и так далее, вот этот контекст, он постепенно, постепенно растёт и в какой-то момент кончается. И

[00:40:00] после этого, а, результат работы последующий вообще не гарантирован, но, строго говоря, он становится гарантирован, он становится негарантированным гораздо раньше. То есть в дополнении к тому, что у нас есть ограничение контекста, которое физическое вполне себе, а в том плане, что оно, а, ну, тупо токеными, количеством их ограничено, у нас есть ещё внимание модели. Внимание - это такая штука, а, которая, по сути, ну, примерно как у людей работает. Точнее, как работает она не как у людей, она проявляется, а, похожа на то, как это устроено у людей. Работает она, конечно, по-другому. А как это вообще, как можно выразить антенtion в этом случае? Ну, у людей есть такое магическое правило, правило 7 п мид. Ну да, по-моему, так. То есть мы можем запомнить семь одновременно понятий держать в голове, а у у кого-то пять, у кого-то девять. Ну там колеблется от пяти до девяти. Вот эффективно мы можем жонглировать в среднем семью, а, одновременно удерживаемыми в голове понятиями. А, и мы склонны забывать некоторые из них по мере роста ��х количества, можем путаться и так далее. Модель ведёт себя внешне примерно так же. То есть, а, есть вот у неё некоторое количество концепций. Это сложно, да, просто сразу скажу, это сложно выразить в виде какой-то формальной метрики. То есть мы не можем сказать, что вот то, что вот сейчас LM видит, вот эта вот концепция, которая является неким юнитом её внимания, мы не можем вот этих вот вещей выразить, а в виде какой-то формальной метрики. А нам для этого никаких инструментов н��дёжных не предоставлено, особенно с учётом того, что многие из моделей являются проприетарными, и мы внутрь них вообще залезть никак не

[00:42:00] можем. Мы можем такие эксперимент ставить на openсоурсных моделях, но вот в случае с пропритарными у нас просто такой возможности нету, к сожалению. Вот. И, соответственно, по мере того, как контекст растёт, количество вот этих вот штук, вот этих вот, мы будем их называть неких концепций, оно тоже увеличивается, то есть их вот становится больше. Аа она там, например, что-то вычитала в проекте, и вот какие-то новые штуки у неё там в памяти появились. Но проблема в том, что количество внимания у неё, оно примерно такое же остаётся. Оно просто растягивается на это, на всё. Вот. И в какие-то моменты у неё просто возникают вот эти вот лаконы, вот эти вот м пробелы в понимании, которые выражаются в галлюцинациях. То есть для модели вообще как бы нет понятия, я чего-то не знаю. Она эффективно заполняет вот эти пробелы в появившемся понимании проекта, в упущенных понятиях теми галлюцинациями, которые у неё есть. Ну, собственно, галлюцинация - это не бага, это просто, скажем, неотъемлемая часть работы модели. А вот она просто эффективно, статистически дополнит тот кусок, который у неё выпал из памяти, тем, что она считает наиболее вероятным, а, на основании того датасета, который у неё был. И, соответственно, мы получаем проблему того, что внимание модели по мере роста само��о контекста, оно становится всё хуже и хуже. И это приводит к проблемам задолго до того, как мы упираемся непосредственно в физический размер лимита контекста. И вам нужно постоянно помнить о том, что а вам не только контекст не нужно, не нужно достигать его физически, а вам нужно ещё и помнить о том, чтобы модель, ну, скажем, ментально не перегружать аа дополнительными вещами такой некой некой

[00:44:00] сложностью контекста. То есть о сложности контекста вам тоже нужно периодически, точнее, не вообще постоянно нужно думать. Задача может быть маленькой, она может занимать небольшое количество контекста, но при этом быть совершенно не посильной для модели, а просто потому что вот этих вот юнитов, а, скажем, сложности, аа, концептуальных каких-то понятий в этом маленьком контексте сосредоточено заведом больше, чем модель вообще, в принципе, может переварить. Вот поэтому задачи нужно подбирать ещё и по этому фактору. Есть бенчмарки, которые эффе��тивно показывают то, насколько внимание модели падает в зависимости от того, мм, в зависимости от роста контекста и в зависимости от количества тех вещей, за которыми модель нужно следить по мере роста этого контекста. Вот один из таких бенчмарков - это Open AI MRCR. Это бенчмарка по поиску иголок в стокисена, когда модель даётся просто, а, разного размера, разной длины, точнее контекст, и заставляет её найти определённые иголки в этом контексте, которые представляют собой, как правило, определённым образом подготовленные фразы. А и модели нужно найти порядковый номер, а точнее фразу по определённому порядковому номеру. Для этого ей приходится трекать а эту фразу на протяжении вот всего этого контекста. и уметь эффективно извлечь. То есть, во-первых, ей нужно подсчитывать эти фразы, а, во-вторых, нужно уметь указать, уметь извлечь конкретную вот эту фразу, конкретный там вопрос, если уж на то пошло из этого контекста на про��яжении его длины. И вот мы можем видеть, что по мере роста контекста, вот снизу у нас длина контекста, аа модели начинают всё хуже и хуже возвращать результат. То есть вот здесь вот как раз процент а успешных э успешного возврата этого результата. И здесь мы говорим о двух иголках. В

[00:46:00] случае с восьмью иголками ситуация намного более драматичная. То есть это вот как раз та самая сложность контекстаtion, а количество внимания модели. А оно уже на старте довольно низкое и падает вообще вот здесь вот в случае с вот этой вот моделью, например, аж до 1.5. А по сравнению с тем, как если бы она вот была просто полностью внимательна, с этим уже невозможно работать. То есть фактически вы понимаете, что при такой работе с контекстом модель уже себе нафантазировала о проекте очень много всего. И это как раз проблема того, что модель перегружена теми вещами, которые, а, вообще ей просто сложно осознать, просто потому что у неё есть некоторые некоторый лимит внимания, который она исчерпывает по мере роста контекста и по мере роста количества вот этих вот иголок, которые находятся в контексте. Вот. А, и при этом модели довольно-таки сильно отличаются по этому фактору друг от друга. Вот, к примеру, сверху здесь указано, а верхний график - это GPT5, а нижний график - это Sonet 4 Thinking. А SanT неплохая модель, но вот вниманием у неё всё-таки дела не настолько хороши, как у G55. А G55 этим и отличается, что она очень хорошо держит контекст на протяжении вот вашей беседы с ней. А, соответственно, если у вас стоит задача именно того, чтобы модель хорошо соблюдала ваши инструкции, хорошо помнил о том, о чём вы с ней говорили на протяжении вот вашего вашей какой-то сессии работы, то вот подобного рода бенчмарки, они довольно эффективно показывают, насколько модель именно в этом плане хорошо будет справляться. Это не единственная характеристика модели, при которая влияет на результаты успешности там или неуспешности её работы, но это довольно важная характеристика. Удержание контекста и эффективность работы с с ним - это одна из важнейших

[00:48:00] характеристик вот конкретной модели. Вот. Потому что а иначе у вас она просто будет делать не то, что вы ей сказали, если она плохо с контекстом работает. Вот это что касается контекста. >> Угу. >> Максим, вклюсь. вопросы. >> А, да, да, давай, знаешь что? Давай базовые расскажем какие-то принципы. Вот сейчас, прежде чем ты пер��йдёшь к субагентам, базовые принципы по работе с контекстом, по его экономии, потому что я вижу в чат люди пишут, там вот у кого-то 50 сообщений, накапливается переписка, у кого-то 100 сообщений. И вот вопрос есть про отличие clear от compact, да, команды. Давай вот базово здесь вот расскажем какие-то такие простые вещи. >> Угу. Ну да. То есть, а, в консольных инструментах как раз в них вот есть, э, ну, вот у меня сейчас вот код запущен, давайте мы, точнее, не запущен, давайте мы его запустим. Я вам просто, а, да, я же, собственно, хотел же про контекст как раз показать. Ну да, для начала можно, например, контекст вообще визуализировать, если уж на то пошло. А в конкретном инструменте, вот, например, в кладкоде, он визуализируется вот таким образом. То есть там вы можете вызвать соответствующую команду, которая вам покажет, что конкретно у ��ас в контексте сейчас находится. Давайте-ка вот так вот сделаем чуть-чуть поменьше. Что-то у меня тут эти этить идти быстро идти ещё быстрее. D. Вот так вот контекст выглядит. Можно, в принципе, посмотреть, что что конкретно сейчас у модели в контексте находится. Давайте к этому вернём всё позже, но это вот по части инструментов работы с контекстом. Clear очищает его вообще полностью, но он не очищает, конечно, его от системного промта, не очищает от системных тулзов, не очищает там от каких-то вот базовых вещей. Вот сейчас контекст, если мы возьмём клир, он, в принципе, таким же останется, просто потому что это прямо первое, что модель прочитала, то есть это то, с чего мы начинаем. Э и есть компакт.

[00:50:00] Компакт - это такая штука, которая вам позволяет по мере, а после того, как вы достигли определённого, а, размера контекста, и особенно, если вы приближаетесь к концу аа вот этого контекста, он вам позволяет его сжать, выкинув из него то, ��то не является существенным для продолжения работы. И даже более того, а-а, здесь вам можно указать, а то, что следует оставить в контексте, то, с чем вы хотите продолжить работать. Ну, например, там continue fixing unit tests, к примеру. Вот если вы до этого фиксили unнитест и у вас закончился контекст и вам хочется сохранить именно информацию о том, что вам нужно для продолжения работы в рамках этой сессии, вы можете просто словами написать: "Хочу", продолжить работу с юнитестами. И специально вот этот вот процесс компактизации, который, кстати, тоже отдельный LM делается. То есть фактически лмки передаётся контекст вот ваш текущий, а и она его суммаризирует как может. А, насколько я помню, там используется для этого Хайку. Легковесная модель от антропиков. Она не очень умная к сожалению. И вот этот вот процесс компактизации - это не то, на что на стоит э стопроцентно надеяться для того, чтобы у вас сессия эффективно продолжилось. Даже более того, то есть это скорее вынужденные меры, а которые бы лучше избегать. То есть лучше бы подбирать задачи так, чтобы вам компактизация не понадобилась. Если уж на авто пошло лучше подбирать те задачи, а кот��рые не приводят к драматичному падению, а внимания модели задолго до того, как ваш контекст кончается. То есть, а компактизация - это уже как бы последняя мера, а желательно до неё не доводить, но тем не менее это есть, оно

[00:52:00] работает. Если вы захотите, вы можете компатизировать контекст, а в том случае, если он приближается к завершению. Вот. А и давайте просто покажу, как вот вот вот вот эта вот диаграммка. Что это вообще такое? Почему почему оно интересно и интересно туда и на ��то поглядывать, что конкретно у вас лежит в контексте? Ну, то есть в контекст попадают такие вещи, как системный промпт. Само собой, вы можете посмотреть даже, сколько он занимает. Вот системные тулзы - это как раз вот всякие read, файл, graп и так далее. То есть вот то, что а системные инструменты, которые поставляют самим агентам, клодкодам, а то, что даётся модель. Вот MCP, а набор MCP серверов и и их инструментов. За этим нужно аккуратно следить, потому что MCP вот особенно в последнее время стало почему-то появляться много статей. То, что MCP плохо, я не не очень понимаю, но просто не надо их добавлять много в проект и всё. То есть фактически MCP - это такая штука, которая вам позволяет расширять возможности модели, но при этом он ещё, точнее, описание инструментов MCP сервера занимает некоторое количество места в контексте. И в некоторых случаях, особенно если вы добавляете их слишком много, они путают модель, у неё внимание уже на старте падает, так что дальше ей довольно-таки тяжело нормально продолжать работать, потому что каждый из этих MCP тулов, они довольно насыщенные в плане того, что модель, э, модели тяжело потом разбираться, а какой из из тулов вызвать. Они перегружают её контекст, и вам бы желательно иметь вот эти количество этих MCP тулов поменьше. И MCP серверов не надо подключать много, а в идеале у вас должно быть там их два-три и прият там довольно-таки небольших для того, чтобы конкретно вот под вашу задачу они были нужны. Не забывайте их удалять, те, которые вам стали не нужны, поэтому за этим прямо постоянно нужно следить. Но у меня тут

[00:54:00] вообще сейчас, по-моему, Ну да, подключу на два. То есть как раз Code Life и Контекст 7. Всё, у меня здесь больше ничего нету. И, как правило, больше двух-трёх-то я и не подключаю. Вот. А кастомные агенты, а это как раз соба агенты. Мы пони про них чуть-чуть сегодня поговорим, а может и не чуть-чуть, посмотрим, как по времени. И аory файлы - это как раз вот код код и точнее код код код MD. А, и связанные, возможно, с ним файлы есть, непосредственно сообщения, которые были посланы. Ну и вот, как вы видите, тут практически всё остальное свободное. Полезная штука. Чисто вот так вот иногда глянуть, насколько у вас там контекст себя хорошо чувствует. Как я говорю, жалко, что нету какого-то формального инструмента для определения внимания модели, но вот, по крайней мере, мы можем посмотреть, что с контекстом происходит. Довольно удобно. >> Угу. Спасибо, Максим. Я тоже немножко здесь добавлю. А, во-первых, про компактизацию, друзья. Кому интересно, я в чатик скинул ссылочку, там вот это прямо банк промтов клодкода зареверсили. В общем, в исследующих целях можете посмотреть, кому интересно, как эта компатизация работает, что сохраняет, что не сохраняет. Очень интересно. А, первый момент. Дальше вот тоже возвращаясь вот к этой теме, да, что там 50 сообщений, у 50 сообщений у вот у кого-то, да, из наших участников сегодня, а переписка с клодом. Аа вот для меня немножко удивительно, да, мне не совсем понятно вот как вот как как это разрастается до пяти сообщений. - ну очевидно, что что-то действительно что что-то не то. Э в по по своему опыту я, ну не знаю, может быть, до пяти сообщений. Ну, может быть, совсем в редких случаях немножко больше может быть, просто потому что действительно очень плотно забивается контекст. Дальше идёт компатизация. И компактизация, вот я сейчас промт скинул. Ну, друзья, как бы она классно не работала, всё равно надо понимать, что она, конечно, многие вещи, многие детали она просто будет упускать, она будет это, э, ну, просто

[00:56:00] явно убирать, да, из промта. А, и, конечно, это дальше качество будет деградировать всё сильнее и сильнее из-за этого. Ну вот. А поэтому вот тоже важно здесь сказать, да, что как только вы понимаете, что у вас вот скоуп задачи, он завершён, да, то есть, я говорю, на моём опыте это, как правило, там до пяти сообщений. Вот я понимаю, что Ага, скоп у меня завершён. Всё. А или вы понимаете, что вы можете, в принципе, а вот сейчас на этом этапе, да, скажем так, контекст очистить и дальше уже продолжить, просто указав файлики, то есть что вы хотите там, да, а-а, что вы хотите, какие улучшения дальше внести, то, пожалуйста, там с clear, всё, пошли дальше, у вас новый контекст и ставите задачу. И это, конечно, будет гораздо эффективно. Это вот первый момент. И второе такой ��ебольшой лайфхак, да, вот есть несколько бенчмарков, я подозреваю, сейчас вот, может, Максим ещё дойдёт до них, как раз вот, которые показывают о том, как разные модели работают с контекстом и насколько хорошо они его держат, да, насколько внимательно они к контексту. А давай, давай сейчас пока начнём с фикшн бенча, Максим, вот там вот сейчас его я хочу про него показать. Да, очень, очень очень интересный бенчмарк там. В общем, идея в том, что, а, вот ребята собрали там некую базу из из всевозможных там, не знаю, рассказов, романов, чего угодно, да, некий фикшн, а, и там, соответственно, некая идёт сюжетная линия. И вот Fiction Bench - это то, как модели, насколько точно они способны проследить за этой сюжетной линией, да, то есть за тем, как менялись персонажи, как какие-то другие происходили изменения и дальше, насколько точно они способны ответить вот на эти вопросы. А и вот здесь вот я в контексте как раз нашего вот разговора на что хотел внимание обратить. Максим, а вот вот здесь вот, да, вот clot s thinking, вот можно посмотреть вот как она держит модель, друзья думающие модели. Вот мы вот здесь важно, да, что а у Сонета и у Опуса есть разные режимы

[00:58:00] работы. А и вот есть режим работы, э, вот этот thinking или reasoning, а там тоже есть разное количество токенов, но давайте условно вот когда вы уrat пишите в чат, да, вот там какую-то выполнить такую-то задачу, think вот сейчас, да, Максим даже визуализирует это и у вас там сразу, если это актуальная версия, у вас ещё красиво это подсветится, да? Вот таким вот образом мы переключаем санеты, мы переключаем опуса вот в этот думающий режим. И давай вернёмся в Fiction Benche. Вот посмотрим, как бы, что нам это даст. То есть что нам это даст? Помимо того, что моделька в принципе начнёт лучше там справляться с какими-то более сложными, более запутанными задачами. Также вот очень интересно здесь можно посмотреть вот как меняется вот сейчас вот 32К или там 60к, да, можно посмотреть 91 94 на 120k 81, да, показатель, ну, какие-то там в попугаях померили, неважно, допустим, это условно показать 81. И если мы, Максим, сейчас вот чуть вниз, давай 120 мы запомнили, 81, давай чуть вниз прокрутим, >> посмотрим Sonnet безing. Вот как как какой там показатель будет. Вот так это Дада. Да, ну прямо покажи, покажи. Да-да, да, покажи. >> Ага. 120. Вот, вот этот, да, по-моему, 36, >> да? >> Ага. Ну, то есть это что это несколько очень существенно. Ну, то есть это вот тоже, да, такой лайфхак. Если вы видите, что у вас там заполняется контекст и вы понимаете, что, ну, как бы он вам очень важен, вы не можете его сейчас очистить. Ну вот, пожалуйста, think или ещё лучше. Чуть-чуть дольше, конечно, это будет работать, но во всяком случае теперь вы, Максим, правильно понимаю, что когда мы альтрацинк пишем, у нас в принципе на все последующие команды вот этот арак передаётся, да, этот передаётся в модели, >> да? Да, должен. Тут вот даже есть отдельно вот у них недавно появилсь такой слш, ну даже не команда, а это слово, которое, я так понимаю, можно вообще в любом месте а промто использовать, который отключает этот режим. То есть он как бы включается и он держится потом на протяжении всей беседы

[01:00:00] и в какой-то момент его можно отключить. Ну да, так и есть. >> Угу. Угу. Да, интересно. Вот там ещё второй чмарк, да, у тебя был там про про контекст. Это как раз MRCR, это он же, >> а, который я на слайдах вот здесь показывал. То есть это просто из него скрины. >> Вот здесь что удобно, вы можете как раз разные модели посравнивать между собой в плане удержания контекста и посмотреть, насколько они в этом хороши. Ну вот у нас сейчас State of the Art модель - это именно GPT5, а она прямо очень хорошо контекст держит. А вот следующая из из интересных мне моделей- это как раз я здесь выбрал GMN 25 Pro и потом вот Clet thinking. А здесь надо понимать, что, конечно, между разными бенчмарками нет смысла сравнивать числа. А надо сравнивать числа в пределах одного бенчмарка и как раз, да, >> модели между собой >> и одной версии. И одной версии весьма одной версии. Да. Дадада. Да, потому что тоже очень сильно всё отличается, потому что даже вот тот же самый, там, не знаю, Sunet 37 и Sunet 4, они будут уже отличаться. Вот. И там вон GPT 4.1 мы возьмём. Это вообще там, ну, она прямо плохая. Вот, соответственно, да, вот удобный бенчмарк, можно заходить смотреть. А при этом заметьте, что вот тот же самый, ну, GNA, у него же контекст до 1 млн токенов, ну, не так, чтобы уж и сильно падает. То есть он довольно-таки уверенно контекст держит. Там, конечно, да, наблюдает, наблюдается там падение, начиная где-то с 256, и дальше оно прямо так сильно начинает падать. Но тем не менее это эффективно работающая более-менее модель с довольно-таки длинным контекстом. Так что, да, это прямо очень хорошо. Это открывает целый ряд сценариев использования конкретной Gemini 2.5 Pro в сравнении с другими моделями. Ну и конкретно вот Google I Studia, а как

[01:02:00] средство по работе с большими контекстами, а-а, там планирование и так далее, прямо очень хороша. Но это, я говорю, целый новый ряд сценариев открывается, когда вот у нас у моделей появляется длинный контекст. И посмотрим, как будет дальше. Ну, потому что сейчас всё-таки бо большинство из них ограничено там где-то двух двумя катокенов, и это прямо печально, хочется больше. Вот. Тём, идём дальше. >> Ой, слушай, а давайте добавим ещё интересный момент. Вот сейчас же появилось э контекста для санета. Вот расскажи про доступность этого всего и вообще про какую-то актуальность, что ли, этого. А, ну да, тут вот в списке моделей, например, есть такая штука, а, как выбор непосредственно модели. И вот здесь вот у некоторых людей начал появляться некоторое время назад Sonet 1M. Это Sonet с одномиллионным контекстом. Я так понимаю, что а это некая такая демовозможность, которая не всем пользователям доступна. И, как правило, она доступна на планах Max. А, то есть там от 100 до 200 баксов в месяц на этих планах. По крайней мере, только от этих людей я слышал, что он у них появился. И это пока довольно-таки небольшое количество пользователей, даже даже среди пользователей вот этого вот плана Макс от антропиков. А те люди, которые с ним работали, говорят, что всё-таки очень сильно контекст проседает по мере роста его длины. Но это буквально, а, пару отзывов я таких встречал на Редите, потому что, опять-таки, повторюсь, не так, чтобы уж большого количество людей доступ к этой модели есть. Это вам позволит, конечно, в неё запихать довольно много, но вот то, насколько она хорошо с этим под количеством информации потом сможет работать, к сожалению, кажется, что не очень. То есть, если это всё тот же самый сонет с той же самой архитектурой,

[01:04:00] с тоже с тем же самым количеством голов внимания, то есть это как раз головы внимания отвечают за то, сколько количество единиц концепций модели может удерживать у себя в памяти. Аа если это всё тот же самый Sнеet, однозначно он будет очень сильно падать на длинных контекстах. Вот. Но зато появляется возможность его использовать, к примеру, для анализа логов, а там, не знаю, для по��ска какой-то информации в длинных, а файлах в качестве такого облегчённого варианта раг. Ну, чтобы не рага постоянно, то прямо не раг запускать, а просто вот кинул там количество определённое файлов в модель, она там сама нашла, что ей надо. Ну, то есть как бы для таких сценариев, где вот нужен большой контекст, но при этом не нужно большого внимания, да, почему нет? Как бы штука хорошая, да, будем ждать, что на всех раскат может быть. Пока что нет. >> Угу. Да, спасибо, Максим. Я сейчас, пока ты говорил, я вспомнил, что мы же собираемся ещё на YouTube выкладывать, рекламную вставку сделаем, да, что у нас каналы с Максимом есть. Вот, да, у меня каналы Driven Development, как раз вот про всё то, что вот мы сейчас обсуждаем и многое другое. У Максима канал называется этих лит в телеграме. Вот тоже очень оченьочень много полезной информации. Максим вообще прям моментально какие-то выходят новинки. Максим всё обозревает и просто шикарно, поэтому тоже могу всем очень горячо рекомендовать. Всё, рекламная пауза закончена. Максим продолжает. Спасибо. >> Да, спасибо. Вот. А и да, я ещё просто скажу, я не публикую практические новости. не от меня прямо вот типа вот там в тот же день это это довольно редко происходит. Я стараюсь всего попробовать по пошупать руками и потом только это а делать обзоры. Ну а я считаю, что это прямо довольно-таки важно. А и вот у Родиона тоже статьи, они, как правило, тоже через опыт использования. Вот я в целом как бы а советую вам выбирать те информационные

[01:06:00] источники, где люди сами пробуют что-то делать в практике. А-а, потому что даже вот в бенчмарке, вы когда лазите, например, выходит какая-то новая модель, там бенчмарки бьют вообще вот все предыдущие модели, начинаешь с ней работать, не то. Ну, то есть бывает так, что бенчмарки просто обманывают. Бывает так, что люди заблуждаются, а, в плане оценок. А-а, короче, выбирайте всегда практ��ков, то есть те люди, которые непосредственно практикуют и что-то делают сами руками. Вот. Так что, да, советую канал Радиона тоже в этом плане у него интересно. Вот так. Ну давайте тогда пойдём дальше. Смотрите, мы с вами поговорили бы про контексты, мы с вами проговорили вообще про то, как агент устроен. И я упомянул здесь вот, когда мы рассматривали анатоми агента, а вот это вот инструмент task, который нам позволяет создавать суббогенты. И вот давайте про них подробнее теперь поговорим. Это вообще как бы одна из таких самых важных фишек колод-кода. А, и они были инноваторами, скажем так, в этом плане. И сейчас вот эта вот фича субагентов начинает появляться в других а-а агентах. И, ну, она по-разному везде реализована, где-то хорошо, где-то плохо. Но вот именно каноничный вариант реализации сейчас только в клодкоде присутствует. И сейчас я объясню, как это работает. То есть, как я уже говорил, инструмент Task запускает э-э поток исполнения внут��и основного процесса самого агента. И этот поток исполнения фактически представляет собой такой мини-агент. А он, в принципе, той же он работает по той же логике, как и основной поток исполнения, основной агент, но при этом он обладает собственными собственным контекстом, собственным циклом жизни, собственным набором инструментов, а, своей системной инструкцией и своим промтом, который ему даёте не вы, а этот промт ему даёт

[01:08:00] основной агент. То есть как это вообще происхо��ит? Основной агент по мере своей работы, он, например, накопил там какое-то количество контекста, что-то поделал, аа и в какой-то момент решает, что у меня есть кусок работы, который я могу скинуть на субагента. Он либо ищет из заданных вами списка субагентов тот, который подходит для конкретной этой задачи, либо сам решает, что кажется, что я могу ответить субагента для того, чтобы передать ему эту задачу. Он этого субагента ответляет, передаёт ему задачу, передаёт необходимый контекст для решения этой задачи. Это, как правило, контекст, а, полученный из самого основного агента. Вот. И субагент на старте получает вот это вот и задачу, и контекст, и начинает свою работу. После того, как он свою работу осуществил, он дошёл до конца, потратил некоторое количество своего контекста, сформировал результат и вернул его в основной агент. А, и вот как раз вот этот зелёненький прямоугольник - это то, что это результат работы собагента. В чём здесь основное преимущество субагентов? В том, что он исполняется независимо от контекста основного агента. И в процессе своей работы он может потратить столько же контекста, сколько и основной агент, то есть до вот до 256 токенов, к примеру, и при этом не засоряйте контекст осново основного агента. Соответственно, самое главное преимущество собагентов в том, что они вам позволяют, а, кратно увеличить контекст основного агента, а, для того, чтобы ему продолжать работу. То есть, если у вас есть задача, точнее, ну как не у вас, у агента есть задача, которую он может передать субагенту, и при этом субагент там, ну, предположим, он тоже

[01:10:00] полазит по файлам, повыполняет какие-то инструкции, там, не знаю, какой-то код напишет, может быть, его протестирует и так далее. То есть, может быть, просто полезет в интернет, что-то какую-то информацию соберёт, проанализирует её и вернёт Сари в основной агент. Это огромная экономия контекста основн��го агента. Соответственно, основной агент в данном случае выступает неким оркестратором, и он может в этом случае гораздо более долгие задачи выполнять и гораздо меньше размывать своё собственное внимание, а, эффективно доделывая длинные, многошаговые и, казалось бы, довольно-таки контекстно интенсивные задачи за счёт использования субагентов. Вот. И суббогентов может быть вызвано несколько в процессе работы. И вы можете даже видеть, пока, например, тот же самый код код работает, можно видеть, как он сам по себе вызывает субагенты. А и при этом, что интересно, он иногда их вызывает в параллель. То есть есть вот возможность вызова субагента в параллель, когда у агента основного появляется некая задача. которая подразумевает то, что нужно, точнее, как она более-менее однотипная и при этом, точнее, нет, не совсем, может быть, и не однотипная, неважно. Суть в том, что иногда у него может появиться какая-то задача, которая позволяет ему, а запустить некоторое количество субагентов в параллель. Он эффективно пилит задачу либо по а отдельным блокам функциональности, либо по а по структуре проекта и запускает сразу несколько субагентов до десяти в параллель, которые каждый справляется со своим выданным ему заданием и в конечном итоге возвращают результат работы в основной агент.

[01:12:00] А, и это, в том числе, экономит ваше время в том плане, что это работает гораздо быстрее. К примеру, если у вас есть задача собрать, э, информацию о том, как работает какая-то фича в проекте или собрать контекст для задачи, вы можете просто основному агенту дать задание собери контекст, а там или найди мне информацию по конкретной фиче с использованием параллельных субагентов. И он их запустит довольно-таки много для того, чтобы непосредственно эту информацию собрать. Ну давайте попробуем какую-то вот такую демку сделать, а, и посмотрим, как это у нас сейчас работает. То есть у меня вот в проекте есть некая materials фича, и я просто явным образом прошу у агента собрать информацию об этой фиче, используя параллельные таски. Вот. И да, вот он сейчас должен начать эти таски запускать. Да, вот вы видите, как раз Таск, таск, таск, таск. И это сейчас идёт параллельное исполнение. Он запустил несколько субагентов, они все одинаковые. Он просто передал им разные части проекта на анализ. То есть вот он в одном таске ищет файлы, относящиеся к материалам, а анализирует отдельно имплементацию бэкэнда, анализирует отдельную имплементацию фронт-энд, а делает ревью схемы базы данных и проверяет апетипы, относящиеся непосредственно к этой фиче. Каждый из этих субагентов, а, смотрит в разные части проекта, собирая информацию о вот этой вот materials фиче для того, чтобы их потом все эти результаты передать в основной агент. Это довольно-таки крутая возможность в том плане, что, ну, во-первых, оно в параллель, а оно быстрее, оно экономит токены основного агента. А основной агент, как как он вообще вот это разделение сделал? Ну, у него есть планировщик, который а тоже обращается клм, а, спрашивая её:

[01:14:00] "Как бы мне распилить эту задачу на параллельных агентов для того, чтобы можно было их независимо друг от друга запустить?" Планировщик, в принципе, довольно эффективно, как правило, решает, как распилить эти задачи, чтобы передать агентам. И в данном случае конфликта между агентами не возникает, потому что они работают редонли, то есть они чисто собирают контекст. Вам единственное, о чём нужно думать, когда вот такие вот штуки возникают, чтобы у вас субагенты не делали какую-то работу, которая из-за которой они потом могут друг другу мешать. Скажем, вот сейчас они в редоль режиме работают, а можно их запускать и в режиме написания кода. А, и в этом случае не надо бороться с планировщиком. Он иногда может решить, что я лучше для этой задачи запущу запущу один субагент, который будет писать код, и он, скорее всего, будет прав. Аа потому что если бы он запустил несколько субагентов для написания кода, вполне возможно, что они бы друг другу мешались, а и там, не знаю, приводили к конфликтам, а, меняя один и тот же файл, а-а, в одном и том же месте и просто мешаясь друг другу. Потому что субагенты, они изолированы друг от друга, и они даже изолированы от основного агента, потому что, как я говорю, у них, во-первых, свой собственный контекст у каждого, а во-��торых, у них нет возможности общаться друг с другом. Это можно сделать через ряд костылей, а, но это не штатная возможность. Надо понимать это, что это такой путь, ээ, как бы, ну, вам придётся чуть-чуть бороться с инструментом. Вот это возможно, но лучше так не делать. А без крайней на то необходимости. То есть, так как агенты между собой не общаются, субагенты между собой не общаютс��, вам бы лучше им давать те задачи, которые хорошо параллелизуемые. Вот. И при этом, а, они не приводят к конфликту за использование

[01:16:00] одного и то же того же ресурса. И в том случае, если вам нужно действительно что-то такое в шир сделать, вот широкие задачи, а или функционально независимые друг от друга задачи, которые возможно выделить в отдельный контекст и где нам не нужно вот этот вот собранный контекст субагента получать обратно в основной агент, а достаточно всего лишь самаре. Это вот тот самый случай, где вам нужны субагенты. Их действительно стоит использовать. А, и нужно исходить именно из этой мотивации. То есть вам не нужно создавать функциональных субагентов только лишь из-за того, что вам так захотелось. Исходите из того, что а вам нужно экономить контекст основного агента. У вас есть параллелизуемые задачи, аа задача, которую можно ответвить эффективно, чтобы она потом вернула чисто сам или в основной агент. То есть надо исходить из экономии контекста, нежели из того, что вам а захотелосьделать каких-то функциональных агентов, которые там каждый выполняет свою непосредственную часть работы. И что ещё интересно, то, что субагенты экономят ваше время. То есть вот каждый из них сейчас в проле, вот видите, выполняется там уже по нескольку минут. То есть сложить вместе, но у нас довольно-таки ощутимое количество времени по получится. Но так как это в параллель, мы, естественно, экономим своё собственное время, но мы не экономим токены. Если у вас оплата, а за инструмент идёт по токенам, ну, субагенты - это, пожалуй, довольно неэффективное решение а подобного рода задач. Имейте это в виду. И в тех случаях, когда вот вы работаете именно с субагентами, а, и когда вам в шире нужно тратить довольно-таки много токеннов. ��от, кстати, можете посмотреть, тут

[01:18:00] довольно-таки много, тут 100, но тут под 200 токенов. Он просто 200 к токенов потратил вот чисто в рамках работы этих субагентов. Это довольно много. Вот. А если вы работаете с оплатой за токена, ну, подходите к этой фиче аккуратно. И хорошо бы вам иметь подписку именно на хорошие тарифные планы, то есть там, которые от 100 там 100-200 баксов стоит, а в которых вам практически не нужно думать о расходе токенов, а там очень щедрые лимиты, что у Кудкода, что у кодекса. И там вы можете использовать вот подобные вещи, токен интенсивные задачи решать, не особо думая о том, как у вас там непосредственно, сколько вы токенов потратите. Вот. Ну и вот он собрал информацию о фиче, то есть в каждоый из этих субагентов, а, предоставил свою сам или в основной агент. И я получил информацию о том, конкретно, как работает у меня materials фича аа в приложении, аа, исходя из разных как раз кусков проекта, где она реализована, то есть как она вообще работает именно сквозь весь проект. Вот. Аэ, в один поток это собиралось бы долго, и у меня бы здесь убился контекст. Мне бы понадобилась компактизация уже давным-давно, и, в общем, часть информации бы, скорее всего, просто из из-за ограниченности внимания модели была бы потеряна. Вот можно собирать контекст, так, а можно пользоваться контекст эннджинами. Вот как бы тут или мемоory бан организовать как-нибудь хорошо, потому что, ну, вот это вот как бы с этой точки зрения крайне неэффективный способ, э, сбора контекста, но он работает. Вот это лишь одно из применений субагентов. А субагентов можно использовать и для написания код, как я уже говорил, и для каких-то вещей, связанных с организацией workflow. А и начиная с какого-то момента, антроopic стали

[01:20:00] позволять не просто в параллель субагентов запускать, потому что раньше вот этот вот инструмент таск, он был не кастомизируемым. Вот сейчас он кастомизируемый, и на его ��снове, на основе субагентов стало возможным делать специализированных субагентов. И про них я сейчас подробнее расскажу. А возможно просто сейчас есть какие-то вопросы, я бы мог на них ответить. >> Да, Максим, вопросов немало. Я только позволю себе на правах вот как бы соведущего. Такой вопрос сразу про контекст этих субагентов. Всё-таки давай поясним, а что конкретно туда входит. Вот, например, clot md файлы, они подсасываются к субагентам контекст, >> насколько я знаю. Да, да, должны. >> MCP. Что ещё? >> MCP, да, MCP. Причём, что интересно, вы можете для конкретного специализированного собагента, а, ему даже указать, какой ты MCP сервер хочешь конкретному субагенту передать. Вот. То есть как бы надо вот вот что понимать. То есть есть субагенты, которые запускает сам код. И это фактически кола instrument task. Он это не специализируемый субагент. Вот сейчас он то, что он запускал- это не специализированный субагент, скажем, это встроенный а субагент, который вы никак поменять не можете. Специализированные субагенты вам дают возможность их кастомизировать в виде вот таких вот MD файлов, положенных в проект. И там вы можете настроить набор инструментов, которые ему передаётся. И одним из таких инструментов является как раз, а такими инструментами, в том числе являются инструменты инструменты MCP серверов. То есть вы при желании можете в конкретный субагент передать набор а конкретных MCP тулов, которые вы хотите, чтобы он ими пользовался. Вот, соответственно, да, туда передаётся, да, насколько я понимаю, то есть субагенты должны

[01:22:00] вычитывать клодм MD, и субагентам передаётся часть контекста основного агента для того, чтобы они могли свою работу осуществлять. То есть вот это так работает, да? То есть MCP там тоже работает. >> Так, да, Максим, спасибо. Несколько вопросов ещё есть в чате. Я бы только попросил, Максим, у тебя доступ есть к чату? >> Да. >> А в 2016 вот Артём спрашивал, вот там вот начинается там череда вопросов по субагентам. Вот посмотри, пожалуйста, а то мне сейчас нужно немножко отвлечься. Здесь >> один и тот же субагент может быть запущен несколько экземпляров, а в нескольких экземплярах параллельно. Вот с этого вопроса начни, пожалуйста. >> Ага. Ага. Ага. >> Нашёл. >> А, да, может один и тот же субагент вполне. То есть вот мы как раз видели вот это�� вот наш неспециализированный дефолтный, скажем так, кодкодовский субагент был запущен в параллель. Аа и там несколько экземпляров выполняется выполнялось. То есть как бы тут вообще никаких проблем с этим нету. Пожалуйста. Вот вот он вот вот вот он как раз работал в параллель. Один и тот же собагент. Так, проверяет ли основной агент результата работы субагентов? Ну как проверяет? он получает результат, но вот фидбэк он никакой дать не может. Дело в том, что субагенты - это ��дноразовая задача, которая будут единожды дозапущенный, то есть он отработал, вернул результат и всё. То есть никакого состояния не сохранится, он просто фактически уничтожается после того, как он закончил работу. Вы, если очень захотите, конечно, можете его контекст, а под конец его работы попросить опять-таки самого субагента, чтобы он его писал куда-то в файл. Но это путь такой крайне недетерминированный, то есть при желании это можно сделать, но это открывает целый другой набор граблей, который вам нужно будет преодолеть. Но чисто теоретически это всё возможно, да? То есть вы можете заставить субагента сохранять свой

[01:24:00] контекст. файл при при окончании работы. Но дело в том, что по по умолчанию в основной агент, он возвращает самари своей работы. И самари этой работы а даже вам не показывается, а оно просто перерабатывается самим основным агентом, используя и используется для его работы. Вот. а-а, какого-то контроля, а, то есть предоставление Фэбка субагенту в процессе не происходит. То есть, как я говорю, это одноразовая задача. У неё есть вход, у неё есть выход, то, что посередине никак на это нельзя повлиять, и он просто уничтожается после того, как он возвращает результат. Вот это так работает. Так, дальше. Нет ли смысла создавать агента, чтобы сэкономить время на промт инженеринг? Ну, я бы не сказал. Там промт составляется самим код-кодом. А, и он, ну, это generic промт, то есть он не так, чтобы уж его хорошо составляет. И если вам нужно детально настроить собагентов, это как раз специализированные собагенты. И вот там вы можете разгуляться уже, как захотите. То есть там действительно очень кастомные промты для собагентов можно прописывать. И мы сейчас на это посмотрим. MCP не удорожает ли токены? Удорожает, да, само собой. То есть вы тратите токены на MCP. Так, тактактак. И что взять? Кодекс или код-код? Ух, это отдельная большая тема. Попробуем её раскрыть под конец. Угу. Угу. Ну да, Игнат пишет, что он использует субагентов, то есть главному агенту поручает выступать оркестратором, валидатором. Он планирует задачу, пилит на томарный подзадачки, делит их на

[01:26:00] группы, которые могут быть запущены параллельно и запускать субогентов параллель. На моей практике работают лучше, чем работа через основного агента. Так и есть, да? То есть, если у вас есть задача, которая позволяет вот её таким образом распилить по этапам и потом обеспечить верификацию работы агента, точе верификацию работы субагента в рамках основного агента, да, но именно, скажем так, если основной агент обнаружил проблемы в том результате, который ему вернул субагент, он, конечно, этот субагент запустить не сможет, дав ему фидбэк. ему придётся новый instance субагента запускать для того, чтобы какую-то проблему пофиксить. Но вот для таких вот вещей, которые так или иначе предполагают многошагость и, скажем, а, выделение части задачи в отдельный контекст, там это хорошо работает. Вот. А вам только единственное, о чём нужно заботиться - это действительно вот об атомарности. Потому что если вы просто набросите какую-то неспецифичную задачу на неспецифичный агент, есть риск того, что он сделает больше, чем надо. А и вот этой вот цепочки у вас просто не получится. Я не знаю, дайдите, например, субагенту. То есть, если у вас субагент, например, программист и фронтend ��рограммист, вы, например, программисту даёте задачу сделать мне для вот этой вот конкретной задачи. А в какой-то момент, э, может случиться так, что колод-код сойдёт с рельсов и сделает вам задачу не только по бэкэнду, но ещё и затронет фронт. Соответственно, следующий агент фронтендер, который будет работать, он будет работать с весьма странным состоянием проекта. И что он там сделает, тоже не очень предсказуемо. Соответственно, вам всегда нужно думать об атомарности и не конфликтовании субагентов одного с другим. Вот особенно если вы их запускаете в параллель. То есть, предположим, есть у вас какой-то субагент, там кэнпрограммист, фрутенпрограммист, вы им даёте одну задачу в параллель,

[01:28:00] и вам нужно быть очень аккуратным, чтобы эту задачу эффективно распилить. Либо за счёт существования каких-то контрактов в проекте, которые вы заранее и тому, и другому даёте, а у вас должен быть хороший набор тестов. Вот. И вообще, в принципе, процедура самой формальной верификации должна быть построена очень хорошо для того, чтобы и каждый из субагентов мог проверить результат своей работы и чтобы основной агент тоже понимал, как а можно проверить результат конкретного субагента. То есть, да, в этом случае основной агент выступает неким оркестратором. Это и эта работа. Вот тактактак. >> Ага. Ага. Вы говорите, да, вот здесь вот у от Михаила, я имею в виду, вы говорите субагент имеет смысл для экономии контек��та или параллельной работы, но ещё кастомный субагент же хранит себе. Да, да, да, хороший пронт. Вот давайте я сейчас как раз про это расскажу. То, что А как вообще принимать решение о том, вызывать субагента или нет? То есть, во-первых, естественно, код, то есть основной агент он, а такие вещи, как, к примеру, запуск юнит-тестов, там, не знаю, поиск по проекту, поиск в интернете, он, в принципе, сам, как правило, чаще всего для этих задач, так как они промежуточные результаты их работы не особо-то нужны в основном контексте, он, как правило, для таких задач сам запускает суббагенту, именно своего неспециализированного субагента, который, а, А, ну, скажем, на все руки мастер. Но если вам нужно делать какой-то специализированный функционал и детально прописать прот для него, используйте кастомных субагентов. Кастомные субагенты - это, скажем, возможность задавать промт а вашим вот этим вот субагентам. не тот, который вот изначально задаётся в рамках дефолтного субагента, а именно тот промт, который вы хотите,

[01:30:00] чтобы задавался. И тут вы можете прописывать какие угодные инструкции. Вот здесь вот у меня в проекте, к примеру, есть субагенты, которые отвечают за за билды, за проверки результатов, за обновление документации. Вот у меня тут библиотекарь есть, я его называю. А есть вот отдельный ресерчер, который может лазить в интернете и искать, например, информацию о библиотеках, которые мне могут понадобиться в пр��екте, и сравнивать их между собой. Вот. И у него, в том числе, есть, например, доступ к контекст 7 MCP, который, а, позволяет последние версии этих библиотек документацию по ним получить. И при этом, что важно, а, и вот этот вот субагент описывается в отдельном файле. У него есть некий набор полей, которые всегда доступны в контексте основного агента. То есть это вот то, что я показывал, когда мы говорили о контексте. Если мы зайдём и посмотрим, что у нас сейчас в контексте лежит, то мы увидим, что вот здесь вот есть custom agents, и сюда входят как раз-таки заголовки, описывающие каждый из субагентов. А то, что мы прописали вот здесь вот в этой секции Yamel from Front Mentor - это специализированный формат описания, а, точнее задание каких-то метаданных в Markдаун файлах через вот некий такой ямл. А здесь описываются некоторые ��оля, которые задают свойства этого агента. То есть у него есть имя, у него есть описание, у него есть набор инструментов, и у него даже можно выбрать модель и даже назначить там цвет, а, который будет использоваться для этого субагента. И в этом случае клод-код сам может принимать решение о том, а не вызвать ли мне субагента, потому что вот я смотрю, что у меня есть

[01:32:00] субагент resarcher, а, который для этой задачи вроде как бы подходит. То есть в том случае, если я, например, аа буду в код-коде писать: "А давай выберем какую-нибудь библиотеку для решения там, не знаю, задачи хранения данных в базе". Вот. А у меня с большой долей вероятности клод-код динамически сам посмотрит на список субагентов, посмотрит, что а вот это вот конкретно субагент подходит для этой задачи, его запустят. При этом там есть возможность через промт, а, через вот этот вот desриption указать код-коду, а, насколько ему, а, как, как бы это объяснить, насколько проактивно он должен использовать этот субагент. Вот прямо ключевые слова у них по документации прямо это написано. Типа вот если вы в описании субагента напишете проектли, он с большой долей вероятности именно будет подбирать, э, точнее он не будет запускать generic агента, он будет запускать именно этого субагента для решения специфичной задачи. Вот. То есть это как раз, а, desрипtion - это способ, э, сказать-коду, используй этого субагента для решения динамически возникшей у тебя задачи. А, но это не единственный способ запуска субагента. Вы можете адресно обращаться к субагентам прямо из контекста основного, точнее, из вашей основной беседы, просто фактически вызывая конкретный агент. Вы можете его просто зареференсить, типа агент Resercher, там, сделай то-то. Аа и он пойдёт и будет выполнять вот эту библиотеку. Он просто запустит таск агентреерчером и будет, э, дальше над этой таской работать. Вы можете делать оркестрации, вы можете, например, сказать, что агент resерчера найти в библиотеку, а потом агент какой-нибудь там библиотекарь, не знаю, добавь информацию об этой библиотеке мне в мой банк. Это тоже

[01:34:00] можно сделать. То есть вы можете в том числе и явным образом говорить лод-коду, как какой из субагентов для какой задачи вызвать. Таким образом, у вас возникает возможность именно создания многошаговых таких пайплайнов и некоторого рода регистрации в рамках основного агента. То есть каждый раз он у вас будет запускать вот этих вот собагентов для решения специфичной задачи. И можно делать это в параллель. То есть, а тут есть, правда, один прикол с тем, что начиная с какого-то времени колод-код начал лениться, запускать параллельных агентов, и вам, возможно, понадобится ему прямо явно говорить о том, что делай что-то, используй параллельные таски. Вот. А-а потому что иногда он ленится, а иногда вам лучше не бороться с его планировщиком, как я уже ранее говорил, потому что вы можете рисковать тем, что у вас, а, каждый из субагентов поменять один и тот же файл, что приведёт к конфликтам. Поэтому с этим надо быть аккуратным. Но если вы чувствуете, что тот набор задач, который вы хотите набросить на ваших субагентов, он параллелизуемый, и при этом они могут работать независимо. Ну вот можно такую штуку использовать, прямо форсить его, используя параллельны параллельные таски или используя а Task Tool. Так тоже работает. Суть в том, что а multiple task Tools, да? А через промтинг этого можно добиться, чтобы он именно в параллелих запускал. А, но как, как я говорю, следите за тем, чтобы это действительно было парализуемо. Так, >> да, Максим, я я здесь. >> А, ну да, там в основном всё там, я так понимаю, комментарии идут. Вот есть люди, кто спрашивает про вот этих твоих субагентов, опять же, да, интересуются, если ты сможешь ими поделиться, может быть, в рамках отдельного репозитория, если ты можешь поделить, >> это отдельно я то есть помнишь, мы как-то говорили, что возможно она может

[01:36:00] сделать прямо практически воркшоп, где бы я просто и прямо и промты показал, рассказал >> при желании, >> да? А, да, а да, а давай спросим, друзья, вот плюсовите. Действительно, насколько было бы интересно вот именно практически воркшоп сделать, насколько тема такая актуальная и сложная, да, одновременно? Ну мы прямо плюсы хорошо собрали. Если бы они ещё бы тогда, если бы всё это в деньги конвертировалось, мы бы с тобой обогатились бы уже, Максим. А так хорошо, да? И смотри, да, про хорошо, мы сказали, что действительно имеет смысл отдельно сделать, Максим, >> и очень сильно надеемся, да, что получится подготовить. >> А, и смотри, там ещё про кодексы уже несколько раз там вопросы поступали, я так понимаю, у тебя тоже как раз будет, да, там сейчас вот такой финальный блок с какого-то сравнения с кодексом �� понимания там, да, когда >> когда вообще он имеет смысл. >> Ага, давай. >> Ну тогда, да, да, да, давай. >> Вот. Да, я на практике могу потом действительно рассказать и про этих субагентов, то есть как, э, лучше промто писать. Вот. Ну, там, на самом деле, довольно тривиально, но вот в динамике это интересно, я думаю, будет посмотреть, как всё это работает. Здесь ещё workflow в самом проекте такой нетривиальный, то есть вот его кусочек здесь показан. Он у меня через GitHub работает. А вот он у меня на гитхабе создаёт сам Ишью, там собирает под них контекст, сам работает, потом сам проверки организовывает. И вот здесь вот субагенты, которые непосредственно всем этим занимаются. Да, можно какой-нибудь воркшоп по этому поводу провести отдельно, посмотреть на практике. Так, >> шикарно. >> А, ну и давайте тогда краткое сравнение с кодексом. То есть, смотрите, код, чем хорош клодкод? Тем, что это каноничная имплементация консольного, а, агента для кодирования. И если вы хотите каноничные функции и

[01:38:00] самыеча фичастости именно в работе, то, конечно, это код-код до сих пор. А почему то есть, как остальные агенты сейчас начинают подтягиваться постепенно, но у всех у них то есть они, как правило, реализуют некое подмножество функций клодкода. И я пока видел очень мало функций, которые бы выбивались за вот это множество тех, а, функций, которые CД-код предоставляет. А, и это немножко парадоксальная ситуация, потому что код на самом деле, ну, несмотря на то, что он, а, проприетарный и его код вроде как бы закрыт, но он написан на тайpesскрипте компили JavaScript, а это означает, что его легко можно декомпилировать. посмотреть, как он внутриустроен, чем, собственно, многие занимаются. Вот. И тут как бы мне не очень понятно, почему некоторые из этих функций, те же самые субагенты, мало где реализованы. В итоге они начинают сейчас вот только появляться в их реализации в других субагентах, ой, в других кодовых агентах. Но это пока всё очень странно выглядит. К примеру, те же команды Козекс вообще сказала, что субагенты у них не в приоритете и сообщество такое как смысле. Ну то есть это одна из тех вещей, которые от команды Кодекс просят прямо с самого начала аа а появления этого инструмента. Непонятно, почему вот люди забивают на реализацию. Где ещё появились субагенты? в Open Code есть в Rode уже, насколько я знаю, тоже есть. Постепенно начинают появляться вот как раз возможности таких вот штук, но они везде. Соответственно, если вы хотите фичатости, используйте код-код. В нём есть куча возможностей, про которые я, на самом деле, сегодня не сказал. Там есть кастомные команды. А, к примеру, вот здесь вот у меня тоже в

[01:40:00] проекте можно видеть их несколько штук, которые, например, относятся непосредственно к работе с Гитхабом. Есть команда, отвечающая за комит, есть команда, которая об��рачивает информацию, а, скажем, собирает информацию, точнее, заставляет прочитать некий набор документов и вообще, в принципе, вводит агента в курс дела за пределами клода MD. То есть команд у меня не так много, но вот они для меня довольно-таки важные в рамках именно workflow этого проекта. Вот куча возможностей, там хуки есть в колодкоде, а тот же самый SDK есть разные режимы думания и так далее. То есть, а все эти вещи, они в том числе в работе довольно-таки часто используются, и они тоже почему-то не везде реализованы, да, они не такие уж сложные в реализации. Мне не очень понятно, почему. Вот. Но, как мы говорили, агент - это не только непосредственно сама программная оболочка, а которая работает с LM. Это, в том числе, и сама LM. Так вот, если сравнивать, а, LLM, то в код-коде а у нас какие там лэмки? Там Sanet и OPUS, да, есть сейчас возможности в код-коде использовать не только Sunet и OPUS, а подключать сторонние лмки. Там от тот же самый Zi есть GLM. Вот есть и другие возможности, то есть подключать, можно подключать какие-то другие нейронки к лод-коду, но это всё как бы, ну, как скажем, создатели клодкода его затачивали на работу именно с нейронками антропика, а работа клодкода с другими нейронками, а, возможно, он, скорее всего, будет с ними работать, но надо понимать, что промптинг, а, который настроен в конкретном инструменте, в том же самом код-коде, они изначально его затачивали на работу именно со своими моделям��. А

[01:42:00] то, как вы строите промт от модели к модели, очень сильно отличается. То есть вы с тем же самым промтом, например, можете пойти в условный чат GPT, получить один ответ, а можете пойти в в какой-нибудь тот же самый клод десктоп, получить другой ответ. Даже более того, между клод десктопом и клоудкодом, используя один и тот же промт, ваш пользовательский промт, вы можете получить разные ответы, просто потому что у них системный промт другой, а между этими двумя разными инструмента��и, и у них какие-то внутренние настройки тоже другие. Соответственно, инструмент код-кода в данном случае затачивается на работу с конкретными, а моделями от самих антропик. И несмотря на то, что технически можно его использовать для других моделей, это вам не гарантирует настолько же успешного результата, как в случае с моделями Anтроopic. И примерно то же самое и с кодексом. То есть кодекс заточен на работу с моделями Open AI. Ну и, соответственно, наиболее эффективно его использовать именно там, то есть именно с моделями Open AI, несмотря на то, что даже кодекс openourсный, и при желании вы можете там сделать fork и какие угодно модели с его помощью запускать, но вот промтинг у него внутренний именно на Open AI модели рассчитан. Так вот, возвращаясь к вопросу, агент - это не только программая программная оболочка, это ещё и модель. Так вот, модель GPT5, которая, с которой работает кодекс, она, как мы уже видели, во-первых, а, довольно-таки неплохо держит контекст. А это всё-таки State of the Art модель. По удержанию контекста она очень хорошо его держит. Она намного более мощная, как то есть, если у вас есть сложные задачи, такие вот как, например, архитектурные задачи или чисто ризининг, а, к примеру, чисто ризинг - это что? алгоритмы, а какие-то сложные пайплайны, которые у вас в ходе организованы, какие-то, может быть, event архитектурные задачи, там прямо вот

[01:44:00] нужен чисто ризининг. И по по ринингу GPT5 очень хороша. Соответственно, вот если у вас есть такого рода задачи, имеет смысл именно GPT5 для них выбирать. Вот. И кодекс, соответственно, как программную оболочку для этого всего. Плюс ко всему, а сам по себе кодекс, а и GPT5 вместе с ним хорошо делают сложные изменения в проектах и именно хорошо работают с существующим кодом. То есть код, пожалуй, лучше всего себя проявляе�� в том случае, когда вам нужно написать какую-то новую функциональность в проектах. Он охотно тратит на это токены, он охотно этот функционал реализует. в какой-то мере довольно-таки неплохо следует инструкциям. Для написа для написания нового кода, особенно массового кода, он очень хорошо подходит. Вот кодексы GPT5 - это инструмент скорее для внесения детальных изменений. А сама модель G5T5 не так охотно тратит токены. Это у неё, видимо, наследство тяжкое O3, которое вообще экономил токен на всём подряд. Вот GPT5 не так экономно в плане токенов, но тем не менее это всё-таки инструмент для детальной для более детальной работы с существующим кодом. Там она хорошо себя проявляет и при этом способна эффективно собирать контекст и эффективно делать выводы по вашему проекту. Вот. А, и она очень хорошо работает с контекстом, как в плане его удержания, так и кодекс как CLI инструмент, выстроенный вокруг модели, он очень хорошо, а, оставляет в контексте только то, что нужно. Вот здесь вот, к примеру, когда у нас контекст растёт, вот тут, а-а, в определённый момент агент, вот эта вот программная оболочка вокруг агента, там cloudкод или кодекс, они могут принимать решение о том, чтобы выбросить какие-то части из контекста, а, прямо в процессе работы модели, даже

[01:46:00] без компакта. И это очень хорошо видно в кодексе, когда вы будете следить за его контекстом. У него там процентный счётчик контекста есть. Он, а, периодически, то есть, например, у вас бывает контекст до 60% дошёл, вы послали сообщение, он скинул его до 40%. Как это происходит? Он просто нашёл какие-то части, видимо, в самом контексте. Может быть, вызов нежных инструментов, может быть, там, не знаю, чтение каких-то логов. Он просто взял их и и повыбрасывал из контекста. То есть это прямо можно видеть в процессе, как контекст по мере работы агента начинает уменьшаться. Это довольно парадоксальная ситуация, но тем не менее вот такое возможно. Они так делают, то есть это именно сами сами агенты этим занимаются. Вот. И как раз-таки вот кодекс а довольно-таки неплохо с этим справляется. Он довольно неплохо чистит контекст в прямо в процессе своей работы. То есть он не только не только модель за счёт своих возможностей хорошо удерживает информацию в контексте, в том числе и агент эффективно выбрасывает из контекста то, что стало ненужным. А надо посмотреть, на самом деле, как это всё работает, потому что это всегда компромиссный такой процесс работы. Можно случайно выкинуть то, что не нужно, точнее, не, да, то, что не нужно выкидывать, но тем не менее такая особенность есть. Вот это вот что по части плюсов кодексы, а вот минусы. А у кодекс у CLI, будь то, либо у кодекса в виде Visual Stud Extension, который ставится, у него всё ещё незрелый тулинг. Тулинг в каком плане? То есть сам по себе кодекс CLI, он гораздо менее фичастый, чем Дкод, как я уже упомянул. И некоторых возможностей типа супагентов там нет. И кажется, что они не особо заинтересованы в том, чтобы их создавать, как ни странно. Не очень понятно, как как они собираются дальше этот

[01:48:00] инструмент продвигать. У меня есть некое ощущение, что команда Open AI собирается выпустить какой-то другой инструмент, не знаю, может быть, более высокого порядка, который будет эти вещи у себя реализовывать. Но вот кодекс CL, несмотря на то, что он очень быстро развивается, то есть там они за пару месяцев добавили столько фич, что, а, ну, для клодкода это требов��ло гораздо больше времени на старте. Вот понятно, что у не у них есть готовый пример у кодекса, но тем не менее они очень шустро его развили, и он действительно хорошо работает. Но вот некоторые фичи, полезные фечи в нём просто отсутствуют, к сожалению. Вот низкоагентность, агентскость - это свойство самой модели. Она не так охотно вызывает тулы. А, но правда это вот сейчас немножко изменилось аа с приходом кодекс вариантов моделей. А, то есть, если вот сейчас будете смотреть, например, тот же самый а кодекс, а кодекс как инструмент, у него там есть список моделей. Так вот, там сейчас появились модели, а, с таким вот суффиксом кодекс. Они больше рассчитаны на агентскую работу, а на меньшую независимость, в отличие от классических моделей GPT5. А, и вроде как действительно оно стало получше, особенно вот если вы используете кодекс high и даёте ему действительно большие задачи. Вот в этом случае у него агентскость повышается, и он дол��ое время может работать независимо, а, вызывая там тулы в процессе и делая какую-то вот работу в рамках чисто вот агентов. Но тем не менее в целом агентскость модели GPT5 она пониже. А сами разработчики кодекса CLI постепенно улучшают агентскость модели за счёт промтинга, но тем не менее пока что это не сравнить с колод-кодом. Если вам нужны агентские задачи, когда у вас

[01:50:00] модель, я не знаю, пошла там запустила какой-то внешний процесс, поработала с MCP, запустила какие-то, а, не знаю, там нагенерила вам код, запустила тесты, сделала билды, я не знаю, ещё какие-то вот многоэтапные такие, а-а, и многосоставные процессы, которые требуют много чего поделать разнообразного. Кодекс пока не настолько хорош в этом плане, увы. И это, как я ещё раз повторюсь, это свойство модели. То есть здесь с этим через промтинг можно что-то улучшить, но вы вы не добьётесь того же самого, что может делать код-код в связке с тем же опусом или санетом. Вот модель просто раньше остановится в плане своей работы. Ну и цены это самый неприятный момент, потому что, например, в случае с клод-кодом у вас есть возможность а купить тариф за 100 баксов. А вот в случае с кодексом у них промежуточного тарифа нет. У них там а либо 20, либо 200 баксов. Аа за 20 баксов вы получаете два счастливых дня работы, как и я их обычно называю. То есть а у вас там даются тарификация ведётся по пятичасовым сессиям. А, и она сейчас я не знаю, он мне Стаци, наверное, не покажет. Ну да, давайте вот так вот сделаем по п по пятичасовым сессиям. И в рамках вот этого двадцатибаксового тарифа вам даётся возможность запустить буквально несколько пяти пятичасовых сессий в рамках недели. Это буквально две-три сессии. Соответственно, после этих двух-трёх сессий доступ у вас блокируется до конца недели. И это прямо большая проблема, потому что а вы реально 2 дня

[01:52:00] поработали с агентом, и остальные 5 дней у вас простой, вы ничего с ним делать не можете. Лимиты очень жёсткие. За 200 баксов лимитов практически нет, но это 200 баксов. Вот. И люди просят то, чтобы добавили какой-то промежуточный тариф, там, не знаю, за 50 или за 100 баксов. Но что-то вот пока команда Open навстречу не идёт. И опять-таки в последней Ask Me сессии на Редити, они прямо сказали, что пока что не рассматривают такую возможность, но просто имейте в виду. То есть если вам с кодексом хочется работать именно комфортно, вам, скорее всего, придётся покупать тариф за 200 баксов. Либо используют какие-то комбинированные варианты. возможно купить тот же самый план а уклодкода за 100 баксов и использовать а кодекс у него есть возможность работать как mcp сервер и использовать его только для вот такого рода задач а из клодкода то есть вы можете использовать cl-код как основной инструмент а кодекс подключить как mcp снаружи и периодически сложные задачи просто на кодекс скидывать в принципе В этом случае двадцатибаксового тарифа вам тоже, ну, довольно-таки надолго хватит, я думаю. >> Ну да, вот можно видеть как раз, как это выглядит. А, Token USA, вот пятичасовой лимит, а, когда он там кончается и вот недельный лимит, который тоже вот, ну, практически у меня не использован, э, потому что у меня, по-моему, вчера неделя началась, я ещё не успел ничего поделать в кодексе. Вот. Но это двухсотбаксовый тариф на двадцатибаксовым. Тут, конечно, всё довольно-таки быстро кончается. Вот просто имейте в виду, что вот, к сожалению, у кодекса нету такого промточного плана и нужно как-то вот какие-то альтернативные варианты искать. Ну и, естественно, использовать какие-то вещи для экономии вот этого вот а юйджа, какие вещи можно использовать. Ну, во-первых, не не позволяйте ему

[01:54:00] забивать контекст, потому что чем длинее контекст, тем быстрее выбираются лимиты по подписке. Чем более толстую модель вы ис��ользуете, тем, опять-таки, быстрее выбираются лимиты. То есть, если вам, то есть для большинства задач на самом деле кодекс medum или там GPT5 medium за глаза. Вот. А в тех случаях, вот прямо, когда реально задача сложная, вам в тех случаях только стоит использовать GPT5 High, а или там кодексхай. Для большинства задач медиум за глаза. Ну и опять-таки хайп просто опять быстрее использует ресурсы подписки. Вот. А-а, ну да. То есть для целевых моментов можно использовать кодекс, а для всего остального CLД-код, по крайней мере, до сих пор пока Open Aim, возможно, не одумаются и не ведут какой-то жичный тариф. Вот как-то так. Это что по части сравнения. Но сейчас, э, по крайней мере, а я для большинства задач всё-таки использую кодекс, но у меня проекты сейчас многие сводятся к тому, чтобы делать аккуратные изменения в большой кодовой базе, а, под моим неустанным контролем. Для этого кодекс хорошо подходит, а он и рефакторинги хорошо делает и так далее. Вот. Ад-код, э, ну, в том случае, если действительно нужно какой-то проект с нуля стартовать, большую фичу запилить где-то сбоку от основного проекта, не вмешиваясь в его основное, скажем, в ядро. Вот. Аа, ну вот я сейчас с кодексом работаю, то есть как-то с клодкода, с колод-кодом поменьше. Вот посмотрим, что Антропик на следующем своём этапе, на следующем витке развития выкатит. Вот. Да, да, Максим, спасибо. Ты как раз в это писал сейчас мой workflow такая рабочая лошадка - это клодкод. И действительно для сложных задач, вот для каких-то бакфиксов сложных, вообще для итеративных каких-то задач на улучшение э по в по фидбеку, это всё как раз вот

[01:56:00] коди кодекс с максимальным ринингом с хай. А это прямо такой классный workflow. А ну давай про кокф два слова скажи. Что уже прямо 2 часа так быстренько. >> Ну да. Коex Cloud - это то же самое, то есть это фактически облачный фоновый агент. Про них можно вообще отдельный доклад делать. И где-то у меня, по-моему, даже есть презентация, которую я давно обновлено делала. А это фоновый агент, который запускается в облаке. под капотом он, насколько нам опять-таки точно неизвестно, но скорее всего там используется GPT5 кодекс medм, а, и это тот, а, агент, который может быть запущен прямо на сайте чаму агенту в веб интерфейсе можете давать задачи, и он их у себя будет где-то в облаке выполнять. Фактически вам нужно предоставить ему ссылку на ваш GitHub репозитори, подклю��ить его там, а он только с Гитхабом работает. А этот агент пойдёт скачать код из вашего GitHub репозитория себе в какой-то временный поднятый токер контейнер. В этом контейнере он запустит а-а аналог кодекс CLI, который выполнит работу в соответствии с той задачей, которую вы ему поставили, а и сделает опционально пулреквест в вашей репозитории, если вы захотите, по окончанию своей работы. В чём прикол конкретно такого режима работы? В том, что он вам позво��яет делать задачи независимо от вашего компьютера. Вы можете, условно, с телефона, там, не знаю, в поездке далеко от дома запускать задачи, которые будут выполняться таким агентом. А, и

[01:58:00] там есть ещё такая прикольная возможность, то, что одного и того же, одну и ту же задачу можно дать на исполнение сразу в параллель нескольким инстансам вот этого вот облачного агента, что довольно прикольно, просто потом можно выбрать лучшие результаты из них. Вот. Но над�� понимать, что это, а, именно изолированное окружение внутри некого его облака. На него накладываются определённые ограничения. Вот. И ваша задача состоит в том, чтобы это окружение правильно настроить, чтобы агент смог эффективно выполнять свою работу. То есть там дать ему сказать ему, как тесты запускать, как билд запускать, аа, возможно, там какие-то библиотеки в этот контейнер добросить. Вот. Но это прямо про него можно просто отдельно рассказать. Там довольно-таки прикольно всё это устроено. И вот таких фоновых агентов сейчас всё-таки довольно много появляется. Они все примерно по одному принципу работают. Можете у меня, кстати, на канале отлистать или отлистать или просто посмотреть содержание канала. Там у меня есть прямо цикл статей про фоновых агентов, где я описываю общие принципы работы. А они никак не поменялись с тех пор. Я где-то в июне, по-моему, это всё писал. Ничего не поменялось. Суть в том, что это, во-первых, позволяет вам в параллель несколько вот этих вот агентов запускать фоновых, а во-вторых, отвязывает вас непосредственно от вашего рабочего места. А где угодно можете это всё выполнять, и это, ну, довольно прикольно, то есть интересная возможность. А модель, да, модель там, судя по всему, GP GPT5 кодекс medum, но нам точно неизвестно. Open AI прямо точно не говорит, что что они там используют. Вот, к сожалению, пока точной информации нету. Вот. >> Угу. Э, давай я быстро дополню тебя тоже своим таким workкфлоу здесь вот по кодекскладу, когда я использую, раньше чаще использовал, сейчас стал реже. Ну вот смотрите, какой для меня кейс самый классный, а с кодексклада, это когда я работаю над какой-то вот там большой сложной задачей, какой-то фичей,

[02:00:00] например, большой. И как в какой подход? Ну, конечно, Spect Driven Development, да, то есть выясняем какие-то нюансы. А, может быть, даже б��ейншторми по кодовой базе там, а как это сейчас устроено, какие тут могут быть корнеркейсы, да? То есть, ну вот, в общем, это, кстати, хорошо через кодалай делать, потому что чатик, всё проиндексировано, всё очень быстро. Так вот, в пользу случаем, да, упомяну. Аа, и далее, когда вот эта спецификация готова, вот по моему опыту как-то вот круче всего вот эту спецификацию закидывать вот в этот кодекс. А почему? Потому что, ну, вот как-то опять же на мой на мой опыт, он как-то наиболее ��втономен. То есть вот вот у меня последняя задача вот то, что я закидывал ему полчаса, 40 минут он выполнял. А-а, и далее, соответственно, ещё отдаёт тебе четыре варианта на ревью. Да, вот это тоже очень мне прямо очень нравится эта фича. Э, иногда можно посмотреть, где больше где больше файлов поменял, тот, наверное, и и лучше, да. Иногда вот так можно совсем просто подходить, но лучше, конечно, повнимательнее посмотреть и действительно выбрать что-то наиболее удачное. Это вот мне такой workflow очень нравится по кодексу. А так, ну, Максим, что у нас по време? Потому что мы уже с тобой больше, ну, ровно 2 часа, да, сначала мы с тобой сидим, у нас ещё пара вопросов есть. У нас ещё есть сколько сколько-ты минуток, Максим? >> Да, в принципе, у меня пока есть. >> Ну да, да, давай тогда, да, сейчас попробуем как-то, может, в режиме блиться, постараться. А, Алексей спрашивает, да, Алексей, привет. Радион Максим, поделитесь, пожалуйста, как вы решаете проблему с тем, что если не следить за каждым шагом, Ии, то всё меньше и меньше начинаешь разбираться в том, что и как происходит в коде проекта. Вот у тебя вот есть такая проблема, Максиму, как ты решаешь? >> А, да, это довольно интересная вещь. А что вас может спасти? То есть, во-первых,

[02:02:00] вы должны держать под контролем высокоуровнюю архитектуру проекта. Вам не должно быть важно, как конкретный код написан. А если у вас есть формальные спецификации на вход и формальные средства проверки того, что написано агентом на выходе, и при этом у вас присутствует хорошее описание и, может быть, формальная архитектура там с интерфейсами, может быть, просто с какими-то контрактами, которые существуют на границах а систем. Вам должно быть не так важно, как конкретный код вообще написан, потому что его в любой момент можно перегенерировать. Если вы чувствуете, что выпускаете и не перестаёте понимать, как конкретная часть системы работает, а просите агентов составлять диаграммы, составлять документацию в компактном виде для того, чтобы вы могли потом прочитать, что конкретно он сделал. То есть под конец работы или в уже написанном компоненте вы просите агента составить документацию по тому, как это работает. Причём важно в отдельной сессии желательно не в той же сессии, где непосредственно он делал эту работу. В отдельном а в отдельной сессии просите составить эту ��окументацию. А смотрите, как это работает, проверяете, соответствует ли это вашим внутренним убеждениям. Но я больше за то, чтобы отпускать контроль за нижними слоями вот этой системы. То есть отпускать контроль за кодом и переходить, а на контроль архитектуры, границ, а и процессов, которые у вас внутри проекта происходят. Вот это вот сейчас становится более важным. Кстати, у нас скоро конференция планируется, и, кажется, я там буду выступать как раз с докладом про архитектуру. применени к вот этому нашему миру вайб-кодинга и насколько важно контролировать как раз именно архитектурные составляющие в проектах в

[02:04:00] сравнении с кодом. Просто это вот тезисы из этого доклада я в том числе сейчас и выдал. То есть а переходите на более высокого уровня контроль. Контроль нужен, но уже не на уровне кода. >> Угу. Да, Максим, спасибо. Я предлагаю потихонечку всё-таки завершать. А-а, смотрите, у нас на самом деле был вопрос изначально там от Евгения о том, что вот каким образом документацию э индексировать и желательно ещё локально. Но я здесь вот кратко отвечу, что, наверное, самое простое, что может быть по индексации документации - это вот как раз вот э загрузить это вот в наш KLIE. Вот. Ну, код он облачный, да, он очень хорошо вам подготовит, очень хороший даст индекс не только по кодум, но и по документации. Вот. А если вы хотите прямо совсем локально, ну, тут тут уж извините, надо смотреть какие-нибудь, может быть, там, а, лонгчейны, да, и вот прочие фреймворки. Их сейчас достаточно много накопилось вот этих локальных рагов. И, ну, тут уже как бы придётся заморачиваться. И, конечно, за итоговое качество по итогу тут, ну, никто не отвечает, потому что сделать рак просто, даже очень просто, а сделать хороший рак - это, ну, это челлендж зачастую. А, ну вот и здесь, и здесь ещё хотел бы тоже добавить касательно вот этой проблемы сложности, проблемы непонимания того, что агенты пишут. Ну, смотрите, здесь вообще есть такое подозрение, что сейчас постепенно мы двигаемся в сторону того, что мы уровень уровень абстрагирования, да, он будет повышаться постепенно, как когда-то он повысился от там э языка ассемблера до э высокоуровневых языков, там условно там C++, потом он условно поднялся C# Java, и сейчас постепенно это поднимается до чего? Ну, как бы до английского, но до такого более формального английского, фактически до вот этих спецификаций. И такое впечатление, что мы сейчас постепенно, да, вот так, если немножко в будущее смотреть, двигаемся именно в эту сторону, что у программистов уже скорее будут спецификации и даже вот там условно отправил там э-э вот отправил

[02:06:00] пиар, да, на кодрев, а приложил к нему спецификацию и ревью уже происходит не не кода, который там получился, а ревю спецификации происходит, да, вот некого подробного текстового описания. Просто просто потому, что оно, как правило, там в разы аа короч��, да, чем тот код, который получился. То есть такое впечатление, что я понимаю, что сейчас немножко для особенно для разработчиков, которые там умею дело действительно с большими системами, это немножко звучит как ээ э- такое, ну, как фантастика некая, да, уже такой фантазии. Но тем не менее пока тенденция такова, что постепенно мы двигаемся в эту сторону. И есть вероятность того, что нам в какой-то момент уже просто так сильно и не и отпадёт необходимость, да, э, собственно, пони��ать именно вот этот вот нескор. Мы же в ассемлер не лезем, мы не лезем там условно там в Ал, да, какой-нибудь, если мы там протишару говорим, смотреть, что во что там оно компилируется. Э так что, как говорится, запасаемся попкорнум. Друзья, всем большое спасибо за участие. Там, возможно, ещё пару вопросов остались. Вот Кагис вот в конце. Максим, если что-то там не ответил, большая к тебе просьба, если возможность будет, может быть, тоже в чатике потом в своём, да, у Максима есть у в канале чат, э, в канале этих лид. Вот Максим там вот на вопросы очень активно отвечает. А поэтому, да, Максим, ещё раз спасибо большое спасибо всем участникам тоже в чаты закидывайте, о чём ещё было бы интересно делать встречи. А напомню, что у нас с Максимом мы ведём каналы в телеграме. Вот у меня и Driven Development на Ютбе в комментариях, я думаю, мы оставим там где-то закрепе у Максима канал этих лит. У нас также есть чатики в этих каналах, поэтому всем спасибо, всем до новых встреч. Максим, если финальное слово есть, тоже скажи. >> Да. Всем всем спасибо, что пришли. 52 человека аж даже осталось. Вот. Спасибо, что доцепили до конца. Ну, надеюсь, получится дальше встречи проводить. Супер. >> Очень круто. Спасибо, друзья.

[02:08:00] >> Всё, всем пока. Хороших выходных. Всё, позвал. Да, всё, всем пока. M.